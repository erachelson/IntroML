{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian models for Machine Learning\n",
    "\n",
    "Suppose we want to predict the probability that sample $x$ has label $y$. This is a probability estimation problem that can be written:\n",
    "$$\\mathbb{P}(Y=y|X=x)$$\n",
    "\n",
    "According to Bayes' theorem, we have:\n",
    "$$\\mathbb{P}(Y=y|X=x) =\\frac{\\mathbb{P}(X=x|Y=y)\\cdot\\mathbb{P}(Y=y)}{\\mathbb{P}(X=x)}$$\n",
    "$$\\textrm{posterior} = \\frac{\\textrm{likelihood}\\cdot\\textrm{prior}}{\\textrm{evidence}}$$\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Bayesian inference** is the problem of estimating this **posterior distribution**.<br>\n",
    "In plain words, it consists in estimating the probability of label $y$, given an input $x$, using previously seen data to estimate the **likelihood** of an $x$ input associated to label $y$ and the general **prior** probability of observing label $y$.\n",
    "</div>\n",
    "\n",
    "Note that Bayesian inference applies both to classification and regression.\n",
    "\n",
    "The goal of Bayesian inference is to estimate the label distribution for a given $x$ and use them to predict the correct label, so it is a *probabilistic approach to Machine Learning*.\n",
    "\n",
    "The Bayesian predictor (classifier or regressor) returns the label that maximizes the posterior probability distribution.\n",
    "\n",
    "In this notebook, we will go through two different Bayesian prediction methods: Naive Bayes Classification and Gaussian Processes.\n",
    "\n",
    "1. [Naive Bayes Classification](#sec1)\n",
    "    1. [The naive Bayes assumption](#sec1-1)\n",
    "    2. [Naive Bayes classifiers in scikit-learn](#sec1-2)\n",
    "    3. [The \"spam or ham?\" example](#sec1-3)\n",
    "    4. [The NIST example](#sec1-4)\n",
    "2. [Gaussian Processes](#sec2)\n",
    "    1. [A reminder on Gaussian distributions](#sec2-1)\n",
    "    2. [Back to the regression problem](#sec2-2)\n",
    "    3. [Gaussian Processes for classification](#sec2-3)\n",
    "    4. [Spam or ham?](#sec2-4)\n",
    "    5. [NIST](#sec2-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id=\"sec1\"></a> Naive Bayes Classification\n",
    "\n",
    "## 1.2 <a id=\"sec1-1\"></a>The naive Bayes assumption\n",
    "\n",
    "Let's start with some illustrative data. We consider an artificial data set of 9 individuals. The first column in our data set is the sex ($S=0$ for male, 1 for female), the second is the height $H$ (in meters), the third is the weight $W$ (in kilos) and the last is the foot size $F$ (in centimeters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig_size=(10, 10)\n",
    "\n",
    "data = np.loadtxt(\"sex_classif.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to answer the question: is $(H=1.81, W=59, F=21)$ male or female?\n",
    "\n",
    "Let's try to estimate $\\mathbb{P}(S=0|H=1.81, W=59, F=21)$.\n",
    "\n",
    "According to Bayes' theorem, the probability that a person that measures 1.81m, weights 59kgs and has a foot size of 21cm is male, is actually the likelihood of observing a person with such features among males, multiplied by the probability of observing males in the population, divided by the probability of observing an individual with these features.\n",
    "\n",
    "That's a long sentence. Let's write that mathematically:\n",
    "$$\\mathbb{P}(S=0|H=1.81, W=59, F=21) = \\frac{\\mathbb{P}(H=1.81, W=59, F=21 | S=0)\\cdot \\mathbb{P}(S=0)}{\\mathbb{P}(H=1.81, W=59, F=21)}$$\n",
    "\n",
    "Let's make that more readable and more general:\n",
    "$$\\mathbb{P}(S|H, W, F) = \\frac{\\mathbb{P}(H,W,F | S)\\cdot \\mathbb{P}(S)}{\\mathbb{P}(H,W,F)}$$\n",
    "\n",
    "Interestingly, since our goal is only to compare the probabilities for $S=0$ and $S=1$, the denominator in the last equation won't be relevant. So we are left with two terms to estimate, given the available data:\n",
    "- $\\mathbb{P}(S=0)$: the prior - the probability that any individual is $S=0$, regardless of his/her physical attributes;\n",
    "- $\\mathbb{P}(H=1.81, W=59, F=21 | S=0)$: the likelihood of meeting somebody with the specified features, given that his/her sex is $S=0$.\n",
    "\n",
    "The prior, in this case, is easy to estimate by comparing the frequence of male and female individuals in the population.\n",
    "\\begin{gather*}\n",
    "\\mathbb{P}(S=0) = \\frac{4}{9}\\\\\n",
    "\\mathbb{P}(S=1) = \\frac{5}{9}\n",
    "\\end{gather*}\n",
    "Technically, the estimate above is obtained by [*maximum likelihood estimation*](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).\n",
    "\n",
    "The likelihood, however, is a bit trickier. Can we directly estimate the **joint probability** of the 3 variables $(H,W,F)$?\n",
    "\n",
    "Theoretically, we can. We can assume that among male individuals, $(H,W,F)$ are distributed according to a multivariate Normal distribution, with mean $\\mu=(\\mu_H, \\mu_W, \\mu_F)$ and covariance matrix $\\Sigma$. The trick is then to estimate $\\mu$ and $\\Sigma$.\n",
    "\n",
    "As a matter of fact, estimating $\\mu$ and $\\Sigma$ without further hypothesis would require quite a lot of data, especially because $\\Sigma$ captures the **correlation** between $H$, $W$ and $F$.\n",
    "\n",
    "Let's rephrase this. With some basic probabilities, we have:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(H,W,F | S) = &\\mathbb{P}(H | S)\\\\\n",
    "& \\cdot \\mathbb{P}(W | S, H) \\\\\n",
    "& \\cdot \\mathbb{P}(F | S, H, W)\n",
    "\\end{align*}\n",
    "\n",
    "Those three probabilities are univariate probabilities, much easier to estimate. However, the first one is a function of $S$ only, the second one depends on $S$ and $H$ and the third one depends on $S$, $H$ and $W$. To get an accurate estimate of the third one, we would need samples of the distribution of $F$ in enough points in the space of $(S,H,W)$ to cover it reasonably. This would require a number of data points that is exponential in the number of variables. That's what is called the **curse of dimensionality**, which makes this estimation problem difficult.\n",
    "\n",
    "Let's make this concrete. Suppose we discretize $H$, $W$ and $F$ in 10 bins each and suppose we require 100 samples to get a correct estimate of $\\mathbb{P}(F | S, H, W)$ for any given value of $(F, S, H, W)$. Then we need $100\\cdot 10^3\\cdot 2$ samples to correctly estimate this probability for all possible values of $(F, S, H, W)$. More generally, if we had $n$ continuous features rather than just three, we would require a number of data points that is exponential in $n$.\n",
    "\n",
    "To circumvent this problem, we are going to make a very **naive** assumption (hence the name of the method). We are going to assume that the weight, the height and the foot size are totally independent variables, that is the probability that a person be 1.85m is the same whatever his/her weight and foot size.\n",
    "\n",
    "Obviously, this hypothesis is very strong and clearly does not hold is most real-world cases. But we will assume it nonetheless. In this case, the likelihood estimation becomes:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(H,W,F | S) = &\\mathbb{P}(H | S)\\\\\n",
    "& \\cdot \\mathbb{P}(W | S) \\\\\n",
    "& \\cdot \\mathbb{P}(F | S)\n",
    "\\end{align*}\n",
    "\n",
    "Each of these probabilities now only depend on the label $S$ and are much easier to estimate from the data. This **conditional independence** assumption is called the **naive Bayes hypothesis**. It allow us to give a (very bad) estimate of $\\mathbb{P}(X | Y)$ and hence of $\\mathbb{P}(Y|X)$.\n",
    "\n",
    "$$\\mathbb{P}(S|H, W, F) = \\frac{\\mathbb{P}(H | S)\\cdot \\mathbb{P}(W | S) \\cdot \\mathbb{P}(F | S)\\cdot \\mathbb{P}(S)}{\\mathbb{P}(H, W, F)}$$\n",
    "\n",
    "Or, in our case:\n",
    "\n",
    "$$\\mathbb{P}(S=0|H=1.81, W=59, F=21) = \\frac{\\mathbb{P}(H=1.81 | S=0)\\cdot \\mathbb{P}(W=59 | S=0) \\cdot \\mathbb{P}(F=21 | S=0)\\cdot \\mathbb{P}(S=0)}{\\mathbb{P}(H=1.81, W=59, F=21)}$$\n",
    "\n",
    "The **naive Bayes classifier** is then the classifier that estimates all class probabilities and returns the one with maximum probability.\n",
    "\n",
    "$$f(H, W, F) = \\arg\\max_{s} \\mathbb{P}(S=s|H,W,F) = \\arg\\max_{s} \\mathbb{P}(H|S=s)\\cdot \\mathbb{P}(W|S=s) \\cdot \\mathbb{P}(F|S=s)\\cdot \\mathbb{P}(S=s)$$\n",
    "\n",
    "Let's implement a naive Bayes classifier on the data above, just to practice. We will assume that the $\\mathbb{P}(X | S)$ distributions are Gaussians (for $X = H,W,$ or $F$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate distribution parameters for males\n",
    "dataM = data[data[:,0]==0]\n",
    "mu_HS0 = np.mean(dataM[:,1])\n",
    "std_HS0 = np.std(dataM[:,1])\n",
    "mu_WS0 = np.mean(dataM[:,2])\n",
    "std_WS0 = np.std(dataM[:,2])\n",
    "mu_FS0 = np.mean(dataM[:,3])\n",
    "std_FS0 = np.std(dataM[:,3])\n",
    "pS0 = dataM.shape[0]/data.shape[0]\n",
    "\n",
    "# Estimate distribution parameters for females\n",
    "dataF = data[data[:,0]==1]\n",
    "mu_HS1 = np.mean(dataF[:,1])\n",
    "std_HS1 = np.std(dataF[:,1])\n",
    "mu_WS1 = np.mean(dataF[:,2])\n",
    "std_WS1 = np.std(dataF[:,2])\n",
    "mu_FS1 = np.mean(dataF[:,3])\n",
    "std_FS1 = np.std(dataF[:,3])\n",
    "pS1 = dataF.shape[0]/data.shape[0]\n",
    "\n",
    "# score that (H=1.81,W=59,F=21) is male/female\n",
    "H=1.81\n",
    "W=59\n",
    "F=21\n",
    "from scipy.stats import norm\n",
    "score_M = pS0 * norm.pdf(H,mu_HS0,std_HS0) * norm.pdf(W,mu_WS0,std_WS0) * norm.pdf(F,mu_FS0,std_FS0)\n",
    "score_F = pS1 * norm.pdf(H,mu_HS1,std_HS1) * norm.pdf(W,mu_WS1,std_WS1) * norm.pdf(F,mu_FS1,std_FS1)\n",
    "print(\"score male    :\", score_M)\n",
    "print(\"score female  :\", score_F)\n",
    "print(\"proba male    :\", score_M/(score_M+score_F))\n",
    "print(\"proba female  :\", score_F/(score_M+score_F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears we will always multiply together values that are smaller than one. Then result will quickly become very small. It is a good habit to move to log-scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_score_M = np.log(pS0) + norm.logpdf(H,mu_HS0,std_HS0) + norm.logpdf(W,mu_WS0,std_WS0) + norm.logpdf(F,mu_FS0,std_FS0)\n",
    "log_score_F = np.log(pS1) + norm.logpdf(H,mu_HS1,std_HS1) + norm.logpdf(W,mu_WS1,std_WS1) + norm.logpdf(F,mu_FS1,std_FS1)\n",
    "print(\"log score male:    \", log_score_M)\n",
    "print(\"log score female:  \", log_score_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: $(H=1.81,W=59,F=21)$ is most probably female.\n",
    "\n",
    "Let's generalize.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "Given $n$ features $X_i$ and classes $Y$, **naive Bayes classifiers** estimate (from data) the distributions $\\mathbb{P}(Y)$ and $\\mathbb{P}(X_i|Y)$. Then, using Bayes rule and the naive Bayes assumption, they predict the most probable estimated class:\n",
    "\\begin{align*}\n",
    "\\arg\\max_{y} \\mathbb{P}(Y=y|X=x) & = \\arg\\max_{y} \\frac{\\prod\\limits_{i=1}^n \\mathbb{P}(X_i=x_i|Y=y) \\mathbb{P}(Y=y)}{\\mathbb{P}(X=x)}\\\\\n",
    "& = \\arg\\max_{y} \\prod\\limits_{i=1}^n \\mathbb{P}(X_i=x_i|Y=y) \\mathbb{P}(Y=y)\\\\\n",
    "& = \\arg\\max_{y} \\sum\\limits_{i=1}^n \\log\\left(\\mathbb{P}(X_i=x_i|Y=y)\\right) + \\log\\left(\\mathbb{P}(Y=y)\\right)\n",
    "\\end{align*}\n",
    "</div>\n",
    "\n",
    "Note that although it is not compulsory to compute the denominator, it is quite straightforward since:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(X=x) &= \\sum\\limits_y \\mathbb{P}(X=x|Y=y)\\mathbb{P}(Y=y)\\\\\n",
    "&= \\sum\\limits_y \\prod\\limits_{i=1}^n \\mathbb{P}(X_i=x_i|Y=y) \\mathbb{P}(Y=y) \n",
    "\\end{align*}\n",
    "So it's the sum of the numerator's values for all $y$, so it's just a matter of normalizing the scores obtained.\n",
    "\n",
    "A really nice thing about naive Bayes classifiers is that it is an **online method**, since most probability distributions can be updated incrementally, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec1-2\"></a>1.2 Naive Bayes classifiers in scikit-learn\n",
    "\n",
    "Once again, scikit-learn has a [naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) implementation. It allows three kind of distributions for the $X_i|Y$ variables: Normal (continuous), Bernouilli or Multinomial (discrete).\n",
    "Let's directly use it on our toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "X = data[:,1:]\n",
    "y = data[:,0]\n",
    "gnb.fit(X,y)\n",
    "xtest = np.array([[1.81,59,21]])\n",
    "print(\"Prediction: \", gnb.predict(xtest))\n",
    "print(\"Probas:     \", gnb.predict_proba(xtest))\n",
    "print(\"Log probas: \",gnb.predict_log_proba(xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 <a id=\"sec1-3\"></a> The \"spam or ham?\" example\n",
    "\n",
    "Let's scale up and apply naive Bayes classification on the ling-spam data. We will assume a multinomial distribution of word $i$ appearing in and email of class $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append('../1 - Text data preprocessing')\n",
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()\n",
    "print(\"data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_nbc = MultinomialNB()\n",
    "spam_nbc.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spam_nbc.score(Xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cross-validation score\n",
    "nb_trials = 20\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = spam_data.shuffle_and_split(2000)\n",
    "    spam_nbc = MultinomialNB()\n",
    "    spam_nbc.fit(Xtrain,ytrain);\n",
    "    score += [spam_nbc.score(Xtest,ytest)]\n",
    "    print('*', end='')\n",
    "print(\" done!\")\n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've trained our model in the Tf-Idf data. Let's see how the model behaves on raw word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')\n",
    "spam_nbc = MultinomialNB()\n",
    "spam_nbc.fit(Xtrain,ytrain)\n",
    "spam_nbc.score(Xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cross-validation score\n",
    "nb_trials = 20\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = spam_data.shuffle_and_split(2000, feat='wordcount')\n",
    "    spam_nbc = MultinomialNB()\n",
    "    spam_nbc.fit(Xtrain,ytrain);\n",
    "    score += [spam_nbc.score(Xtest,ytest)]\n",
    "    print('*', end='')\n",
    "print(\" done!\")\n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify which are the misclassified emails (and find the confusion matrix by the way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')\n",
    "spam_nbc = MultinomialNB()\n",
    "spam_nbc.fit(Xtrain,ytrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "ypredict = spam_nbc.predict(Xtest)\n",
    "misclass = np.not_equal(ypredict, ytest)\n",
    "Xmisclass = Xtest[misclass,:]\n",
    "ymisclass = ytest[misclass]\n",
    "misclass_indices = [i for i, j in enumerate(misclass) if j == True]\n",
    "print(\"Misclassified messages indices:\", misclass_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(ytest, ypredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some misclassified mails\n",
    "index = misclass_indices[1]+2000\n",
    "print(\"Prediction:\", spam_nbc.predict(spam_data.word_count[index,:]))\n",
    "spam_data.print_email(index)\n",
    "spam_nbc.predict_proba(spam_data.tfidf[index,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 <a id=\"sec1-4\"></a> The NIST example\n",
    "\n",
    "We will assume Gaussian distributions for the NIST example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "print(digits.data.shape)\n",
    "print(digits.images.shape)\n",
    "print(digits.target.shape)\n",
    "print(digits.target_names)\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "Xtrain,Xtest = np.split(X,[1000])\n",
    "ytrain,ytest = np.split(y,[1000])\n",
    "#Xtrain = X[:1000,:]\n",
    "#ytrain = y[:1000]\n",
    "#Xtest = X[1000:,:]\n",
    "#ytest = y[1000:]\n",
    "\n",
    "#plt.gray();\n",
    "#plt.matshow(digits.images[0]);\n",
    "#plt.show();\n",
    "#plt.matshow(digits.images[15]);\n",
    "#plt.show();\n",
    "#plt.matshow(digits.images[42]);\n",
    "#plt.show();\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def shuffle_and_split(X,y,n):\n",
    "    X0,y0 = shuffle(X,y)\n",
    "    Xtrain,Xtest = np.split(X0,[n])\n",
    "    ytrain,ytest = np.split(y0,[n])\n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "digits_nbc = GaussianNB()\n",
    "digits_nbc.fit(Xtrain,ytrain)\n",
    "prediction = digits_nbc.predict(Xtest)\n",
    "#print(\"Training error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest))\n",
    "print(\"Generalization error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest) )\n",
    "print(\"Generalization score:\", digits_nbc.score(Xtest,ytest))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(ytest, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cross-validation score\n",
    "nb_trials = 20\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = shuffle_and_split(X,y,1000)\n",
    "    digits_nbc = GaussianNB()\n",
    "    digits_nbc.fit(Xtrain,ytrain)\n",
    "    score += [digits_nbc.score(Xtest,ytest)]\n",
    "    print('*',end='')\n",
    "print(\" done!\")\n",
    "    \n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers reach their limits on data with high correlations between features (like images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id=\"sec2\"></a>Gaussian Processes\n",
    "\n",
    "Suppose we want to learn a certain phenomenon $f(x)$, given some measurements $y = f(x)$ + noise.\n",
    "\n",
    "For example, that could be learning the pressure $P$ across the wingspan of a plane as a function of the position $l$ on the wing, the Mach value $M$, and the temperature $T$ (so $x=(l,M,T)$).\n",
    "\n",
    "Let's suppose that for a given value of $x$, the corresponding observation $y$ follows a Gaussian distribution of mean $\\mu$ and variance $\\sigma^2$.\n",
    "\n",
    "Need a reminder on Gaussian distributions?\n",
    "\n",
    "## 2.1 <a id=\"sec2-1\"></a>A reminder on Gaussian distributions\n",
    "\n",
    "A Gaussian (or Normal) distributed variable has the following probability density function. The mean $\\mu$ is the most frequent value observed for this variable, while the variance $\\sigma^2$ indicates the diversity of values that $y$ often takes (how the distribution is spread-out along $y$).\n",
    "\n",
    "We say that $y$ is drawn from the Gaussian distribution of mean $\\mu$ and variance $\\sigma^2$ and write:\n",
    "$$y \\sim \\mathcal{N}\\left(\\mu,\\sigma\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "mu = 0.0\n",
    "sigma = 2.0\n",
    "x = np.linspace(-10., 10, 100)\n",
    "plt.plot(x, norm(mu,sigma).pdf(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take two values $x_1$ and $x_2$. The value taken by $y$ in $x_1$ is not independent of the value in $x_2$; we shall say that $y_1$ and $y_2$ are **correlated**. For example, the pressures $P$ in close $(l,M,T)$ values take values that are linked together.\n",
    "\n",
    "Let's still suppose the pair $(y_1,y_2)$ follows a Gaussian distribution in $x_1$ and $x_2$, with mean $(\\mu_1,\\mu_2)$. While the variance in the monovariate case was given by a single scalar $\\sigma^2$, now it is given by a covariance matrix $\\Sigma = \\left[\\begin{array}{cc} \\sigma_{11} & \\sigma_{12}\\\\ \\sigma_{12} & \\sigma_{22} \\end{array}\\right]$ that depicts how the distribution spreads in the $(y_1,y_2)\\in\\mathbb{R}^2$ plane.\n",
    "\n",
    "$$\\left[\\begin{array}{c}y_1\\\\y_2\\end{array}\\right] \\sim \\mathcal{N}\\left( \\left[\\begin{array}{c} \\mu_1\\\\\\mu_2\\end{array}\\right], \\Sigma\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "x,y = np.mgrid[-1:1:.01, -1:1:.01]\n",
    "pos = np.dstack((x, y))\n",
    "mu = [0.0, 0.0]\n",
    "sigma = np.array([[2.0, 0.4], [0.4, 0.5]])\n",
    "plt.contourf(x, y, multivariate_normal(mu, sigma).pdf(pos));\n",
    "print(\"mu =\",mu)\n",
    "print(\"sigma:\")\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix indicates how the values of $y_1$ are centered around $\\mu_1$ (through $\\sigma_{11}$) and correlated with the value of $y_2$ (through $\\sigma_{12}$). In particular, $\\sigma_{12}$ indicates how similar the evolution of $y_1$ and $y_2$ are. Suppose we measure a sample $(\\hat{y}_1, \\hat{y}_2)$:\n",
    "- having $\\sigma_{12}>0$ indicates that if $\\hat{y}_1>\\mu_1$ then it is likely that $\\hat{y}_2>\\mu_2$ in a proportional fashion,\n",
    "- conversely $\\sigma_{12}<0$ indicates that $\\hat{y}_1$ and $\\hat{y}_2$ evolve in opposite directions relatively to their respective means,\n",
    "- $\\sigma_{12}=0$ means that $y_1$ and $y_2$ are independent: a sample of the former gives no information about how likely it is that the latter is greater or smaller than its mean.\n",
    "\n",
    "If there are $N$ such points $x_i$, and we assume again a Gaussian distribution of $y$, we end up with a mean $(\\mu_1,\\ldots,\\mu_N)$ and a covariance matrix $\\Sigma = \\left[\\begin{array}{ccc} \\sigma_{11} & \\ldots & \\sigma_{1N}\\\\ \\vdots & \\ddots & \\vdots \\\\ \\sigma_{1N} & \\ldots & \\sigma_{NN} \\end{array}\\right]$.\n",
    "\n",
    "$$\\left[\\begin{array}{c}y_1\\\\ \\vdots\\\\y_N \\end{array}\\right] \\sim \\mathcal{N}\\left( \\left[\\begin{array}{c} \\mu_1\\\\ \\vdots \\\\ \\mu_N\\end{array}\\right], \\Sigma\\right)$$\n",
    "\n",
    "Let's generalize to any number of points:\n",
    "- The mean becomes $\\mu(x)$\n",
    "- The covariance matrix can be obtained through a **covariance kernel** $k(x,x')$\n",
    "\n",
    "For a finite set of points $(x_1,\\ldots,x_N)$, the corresponding $(y_1,\\ldots,y_N)$ follow a Gaussian distribution of mean $(\\mu(x_1),\\ldots,\\mu(x_N))$ and of covariance matrix $\\Sigma = \\left[\\begin{array}{ccc} k(x_1,x_1) & \\ldots & k(x_1,x_N)\\\\ \\vdots & \\ddots & \\vdots \\\\ k(x_1,x_N) & \\ldots & k(x_N,x_N) \\end{array}\\right]$.\n",
    "\n",
    "$$\\left[\\begin{array}{c}y_1\\\\ \\vdots\\\\y_N \\end{array}\\right] \\sim \\mathcal{N}\\left( \\left[\\begin{array}{c} \\mu(x_1)\\\\ \\vdots \\\\ \\mu(x_N)\\end{array}\\right], \\Sigma\\right)$$\n",
    "\n",
    "The information of $\\mu(x)$ and $k(x,x')$ defines a **Gaussian Process**. If we were able to learn a certain hidden function $f$ as a Gaussian Process, not only would we have the average function that fits $f$ best, but we would also have the probability that any other function fits $f$. In other words, in any new point $x$, we would have the average prediction $\\mu(x)$ but also an **uncertainty estimation** through the variance estimate $\\sigma(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec2-2\"></a>2.2 Back to the regression problem\n",
    "\n",
    "So we assume that $y$ is drawn from a Gaussian Process.\n",
    "\n",
    "Recall that $\\mu(x)$ is the mean of the distribution in $x$ and that $k(x,x')$ describes how similar the values of $y$ and $y'$ (resp. in $x$ and $x'$) are likely to be spread around their respective means.\n",
    "\n",
    "To make things simple, assume that the Gaussian Process describing $f$ has mean zero, so $\\mu(x)=0$ everywhere. So what relates an observation to another is just the covariance function $k(x,x')$.\n",
    "\n",
    "A common covariance function is the so-called **squared exponential**:\n",
    "$$k(x,x') = \\sigma^2 e^{-\\frac{(x-x')^2}{2 l^2}}$$\n",
    "\n",
    "With this covariance function, the correlation between $y$ and $y'$ is high when $x$ and $x'$ are close. As $x$ and $x'$ get further from each other, $y$ and $y'$ tend to become independent.\n",
    "\n",
    "Since we assume that the observed data $\\left\\{(x_i,y_i)\\right\\}_{i=1...N}$ is drawn from a Gaussian distribution and that any new point $(x,y)$ should be drawn from that distribution too, we can write:\n",
    "\n",
    "$$\\left[\\begin{array}{c}y_1\\\\ \\vdots\\\\y_N \\\\y \\end{array}\\right] \\sim \\mathcal{N}\\left( 0_{N+1}, \\left[\\begin{array}{cccc}\n",
    "k(x_1,x_1) & \\ldots & k(x_1,x_N) & k(x_1, x)\\\\ \n",
    "\\vdots     & \\ddots & \\vdots     & \\vdots \\\\\n",
    "k(x_1,x_N) & \\ldots & k(x_N,x_N) & k(x_N,x)\\\\\n",
    "k(x_1, x)  & \\ldots & k(x_N,x)   & k(x,x)\n",
    "\\end{array}\\right] \\right)$$\n",
    "\n",
    "(where $0_{N+1}$ is the $N+1$ dimensional zero vector)\n",
    "\n",
    "The line above simply states that all points, both the observed data points and the new points on which we wish to make a prediction, are drawn according to the same Gaussian Process. Let's simplify the writing using vector notation. Let's write:\n",
    "- the data points $\\mathbf{y} = \\left[y_1,\\ldots,y_N\\right]$ and $\\mathbf{x} = \\left[x_1,\\ldots,x_N\\right]$, \n",
    "- the data covariance matrix $K = \\left[\\begin{array}{ccc}\n",
    "k(x_1,x_1) & \\ldots & k(x_1,x_N)\\\\ \n",
    "\\vdots     & \\ddots & \\vdots\\\\\n",
    "k(x_1,x_N) & \\ldots & k(x_N,x_N)\n",
    "\\end{array}\\right]$\n",
    "- The cross-covariance vector $K_*(x) = \\left[k(x_1, x), \\ldots, k(x_N,x)\\right]$\n",
    "\n",
    "Then we have:\n",
    "$$\\left[\\begin{array}{c}\\mathbf{y}\\\\y \\end{array}\\right] \\sim \\mathcal{N}\\left( 0_{N+1}, \\left[\\begin{array}{cc}\n",
    "K & K_*(x)^T\\\\ \n",
    "K_*(x) & k(x,x)\n",
    "\\end{array}\\right] \\right)$$\n",
    "\n",
    "Let's take a step back. What we have written so far is the probability $\\mathbb{P}(\\mathbf{y},y|\\mathbf{x},x)$. But what we are interested in is the posterior distribution $\\mathbb{P}(y|\\mathbf{x},\\mathbf{y},x)$; that is \"the probability of the prediction $y$, given the data $(\\mathbf{x},\\mathbf{y})$\". So let's use Bayes theorem:\n",
    "\n",
    "$$\\mathbb{P}(y|\\mathbf{x},\\mathbf{y},x) = \\frac{\\mathbb{P}(\\mathbf{y},y|\\mathbf{x},x)}{\\mathbb{P}(\\mathbf{y}|\\mathbf{x},x)}$$\n",
    "\n",
    "Well it appears that because we have assumed that the values were drawn from a Gaussian Process, this posterior follows a Gaussian distribution too:\n",
    "$$\\mathbb{P}(y|\\mathbf{x},\\mathbf{y},x) = \\mathcal{N}\\left( K_*(x)K^{-1} \\mathbf{y}, k(x,x) - K_*(x) K^{-1} K_*(x)^T\\right)$$\n",
    "\n",
    "Consequently, our best estimate for $y$ is the mean of this distribution:\n",
    "$$\\bar{y} = K_*(x) K^{-1} \\mathbf{y}$$\n",
    "\n",
    "And the uncertainty in this estimate is captured by the variance:\n",
    "$$\\sigma(y)^2 = k(x,x)- K_*(x)K^{-1}K_*(x)^T$$\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "Given the input data $\\mathbf{y} = \\left[y_1,\\ldots,y_N\\right]$ and $\\mathbf{x} = \\left[x_1,\\ldots,x_N\\right]$, and given a covariance kernel $k(x,x')$, a Gaussian Process regressor estimates the distribution of $y(x)$ as a Gaussian $\\mathcal{N}(\\mu,\\sigma)$ with:\n",
    "<ul>\n",
    "<li> $\\mu = K_*(x)K^{-1} \\mathbf{y}$\n",
    "<li> $\\sigma(y)^2 = k(x,x) - K_*(x)K^{-1}K_*(x)^T$\n",
    "</ul>\n",
    "where:\n",
    "<ul>\n",
    "<li> $K = \\left[\\begin{array}{ccc}\n",
    "k(x_1,x_1) & \\ldots & k(x_1,x_N)\\\\ \n",
    "\\vdots     & \\ddots & \\vdots\\\\\n",
    "k(x_1,x_N) & \\ldots & k(x_N,x_N)\n",
    "\\end{array}\\right]$\n",
    "<li>$K_*(x) = \\left[k(x_1, x), \\ldots, k(x_N,x)\\right]$\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Let's take a concrete example.\n",
    "\n",
    "Suppose the hidden function we want to learn is $f(x) = x \\sin(x)$.\n",
    "\n",
    "Suppose the observation is drawn as $y = f(x) + \\mathcal{N}(0, \\sigma_n^2)$.\n",
    "\n",
    "Let's draw 10 data samples and try to obtain the best fit function and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x*np.sin(x)\n",
    "\n",
    "sigma_noise = 0.3\n",
    "def observation(x):\n",
    "    return func(x) + np.random.normal(0,sigma_noise,x.shape[0])\n",
    "\n",
    "X = np.linspace(-5,10,10)\n",
    "N = X.shape[0]\n",
    "Y = observation(X)\n",
    "x = np.linspace(-5,10,100)\n",
    "\n",
    "fig=plt.figure(figsize=(5,5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(x, func(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(X, Y, 'r.', markersize=10, label=u'Observations')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-6, 10)\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best function, we need a covariance kernel. We will take the square exponential kernel with arbitrary $\\sigma$ and $l$ parameters and will modify it slightly to account for the measurement noise $\\sigma_n$. Then we simply apply the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce the square exponential kernel with measurement noise\n",
    "sigma_kernel = 0.2\n",
    "l=0.3\n",
    "def kernel(x1,x2,sigma_kernel,l,sigma_noise):\n",
    "    return sigma_kernel * sigma_kernel * np.exp(-(x1-x2)**2 / (2*l*l)) + sigma_noise*(x1==x2)\n",
    "\n",
    "K = np.zeros((N,N))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        K[i,j] = kernel(X[i],X[j],sigma_kernel,l,sigma_noise)\n",
    "\n",
    "Kinv = np.linalg.inv(K)\n",
    "\n",
    "def GPpredict(x_new, x_data, y_data, Kinv):\n",
    "    N = y_data.shape[0]\n",
    "    Kstar = np.zeros((1,N))\n",
    "    for i in range(N):\n",
    "        Kstar[0,i] = kernel(x_data[i], x_new, sigma_kernel, l, sigma_noise)\n",
    "    mu = Kstar @ Kinv @ y_data\n",
    "    sigma = kernel(x_new,x_new,sigma_kernel,l,sigma_noise) - Kstar @ Kinv @ Kstar.T\n",
    "    return mu, sigma\n",
    "\n",
    "x = np.linspace(-5,10,100)\n",
    "y = np.zeros(x.shape)\n",
    "sigma = np.zeros(x.shape)\n",
    "for i in range(x.shape[0]):\n",
    "    y[i], sigma[i] = GPpredict(x[i], X, Y, Kinv)\n",
    "\n",
    "fig=plt.figure(figsize=(5,5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(x, func(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(X, Y, 'r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x, y, 'b-', label=u'Prediction')\n",
    "plt.fill(np.concatenate([x, x[::-1]]),\n",
    "         np.concatenate([y - 1.9600 * sigma,\n",
    "                        (y + 1.9600 * sigma)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-6, 10)\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's quite disappointing. Indeed, the (blue) learned curve goes through the data points but it is far from the (red) original function. An interesting feature however is that the uncertainty reduces almost to zero around the data points and strongly increases elsewhere.\n",
    "\n",
    "What happened? It seems we made a poor arbitrary choice of kernel parameters. For instance taking $l=0.3$ implies that values $y$ and $y'$ for two points $x$ and $x'$ quickly tend to be independent and so the \"influence\" of a data point (a red point) does not extend far.\n",
    "\n",
    "Let's re-run this experiment with other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_kernel = 3\n",
    "l=1\n",
    "def kernel(x1,x2,sigma_kernel,l,sigma_noise):\n",
    "    return sigma_kernel * sigma_kernel * np.exp(-(x1-x2)**2 / (2*l*l)) + sigma_noise*(x1==x2)\n",
    "\n",
    "K = np.zeros((N,N))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        K[i,j] = kernel(X[i],X[j],sigma_kernel,l,sigma_noise)\n",
    "\n",
    "Kinv = np.linalg.inv(K)\n",
    "\n",
    "def GPpredict(x_new, x_data, y_data, Kinv):\n",
    "    N = y_data.shape[0]\n",
    "    Kstar = np.zeros((1,N))\n",
    "    for i in range(N):\n",
    "        Kstar[0,i] = kernel(x_data[i], x_new, sigma_kernel, l, sigma_noise)\n",
    "    mu = Kstar @ Kinv @ y_data\n",
    "    sigma = kernel(x_new,x_new,sigma_kernel,l,sigma_noise) - Kstar @ Kinv @ Kstar.T\n",
    "    return mu, sigma\n",
    "\n",
    "x = np.linspace(-5,10,100)\n",
    "y = np.zeros(x.shape)\n",
    "sigma = np.zeros(x.shape)\n",
    "for i in range(x.shape[0]):\n",
    "    y[i], sigma[i] = GPpredict(x[i], X, Y, Kinv)\n",
    "\n",
    "fig=plt.figure(figsize=fig_size, dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(x, func(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(X, Y, 'r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x, y, 'b-', label=u'Prediction')\n",
    "plt.fill(np.concatenate([x, x[::-1]]),\n",
    "         np.concatenate([y - 1.9600 * sigma,\n",
    "                        (y + 1.9600 * sigma)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-6, 10)\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This starts to look better. The blue curve fits the data points better but it's still not perfect and the uncertainty between data points is actually quite large.\n",
    "\n",
    "We could wish for an automated way of tuning these kernel parameters. Let's call them $\\theta$.\n",
    "\n",
    "One way to do that is to search for the value of $\\theta$ that maximizes the probability of the observed $\\mathbf{y}$ given the inputs $\\mathbf{x}$. In other words, to find the best $\\theta$, we search for the one that explains best the data that we already know, and then use it to make predictions on new points. That is called **Marginal Likelihood Maximization**.\n",
    "\n",
    "Fortunately for use, scikit-learn includes all these operations in a single procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "gp = GaussianProcessRegressor(n_restarts_optimizer=9)\n",
    "X=X.reshape(-1,1)\n",
    "Y=Y.reshape(-1,1)\n",
    "gp.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,10,100)\n",
    "y = np.zeros(x.shape)\n",
    "sigma = np.zeros(x.shape)\n",
    "for i in range(x.shape[0]):\n",
    "    y[i], sigma[i] = gp.predict(x[i].reshape(-1,1), return_std=True)\n",
    "\n",
    "fig=plt.figure(figsize=(5,5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(x, func(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(X, Y, 'r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x, y, 'b-', label=u'Prediction')\n",
    "plt.fill(np.concatenate([x, x[::-1]]),\n",
    "         np.concatenate([y - 1.9600 * sigma,\n",
    "                        (y + 1.9600 * sigma)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-6, 10)\n",
    "plt.legend(loc='upper left');\n",
    "print(gp.kernel_.hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's better. Although the correct function was not found exactly, given the few data points we had it's already a good fit.\n",
    "\n",
    "Let's summarize a few properties of Gaussian Processes.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<ul>\n",
    "<li> Gaussian processes are an offline method\n",
    "<li> They are an optimal method (in the sense of Bayesian inference)\n",
    "<li> They provide a best fit function, but also error bounds\n",
    "<li> Given $N$ data points, they require the inversion of an $N\\times N$ covariance matrix (complexity in $O(N^3)$)\n",
    "<li> The kernel encodes the prior knowledge about the function's behaviour.\n",
    "<li> Careful choice / design of kernels can make GPs a very powerful tool... but can also make them very unstable computationally.\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Let's illustrate this last property on a final example, with a periodic function and the \"Exp-Sine-Squared\" kernel that is commonly used for periodic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared\n",
    "\n",
    "# Generate sample data\n",
    "rng = np.random.RandomState(0)\n",
    "X = 15 * rng.rand(100, 1)\n",
    "y = np.sin(X).ravel()\n",
    "y += 3 * (0.5 - rng.rand(X.shape[0]))  # add noise\n",
    "\n",
    "# GP training\n",
    "gp_kernel = ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1)) + WhiteKernel(1e-1)\n",
    "gpr = GaussianProcessRegressor(kernel=gp_kernel)\n",
    "gpr.fit(X, y)\n",
    "\n",
    "# GP prediction\n",
    "X_plot = np.linspace(0, 25, 10000)[:, None]\n",
    "y_gpr, y_std = gpr.predict(X_plot, return_std=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=fig_size)\n",
    "lw = 2\n",
    "plt.scatter(X, y, c='k', label='data')\n",
    "plt.plot(X_plot, np.sin(X_plot), color='navy', lw=lw, label='True')\n",
    "plt.plot(X_plot, y_gpr, color='darkorange', lw=lw,\n",
    "         label='GPR (%s)' % gpr.kernel_)\n",
    "plt.fill_between(X_plot[:, 0], y_gpr - y_std, y_gpr + y_std, color='darkorange',\n",
    "                 alpha=0.2)\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.xlim(0, 25)\n",
    "plt.ylim(-4, 4)\n",
    "plt.legend(loc=\"best\",  scatterpoints=1, prop={'size': 8})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec2-3\"></a>2.3 Gaussian Processes for classification ?\n",
    "\n",
    "It actually works exactly the same way. For binary classification problems, Gaussian Processes try to predict the probability of belonging to the first class (thus turning back to a regression problem).\n",
    "\n",
    "More precisely, they try to build a hidden function $f(x)$ so that: \n",
    "$$\\mathbb{P}(Y=0|X=x) = \\frac{1}{1+e^{f(x)}}$$\n",
    "Where $f$ is a Gaussian Process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec2-4\"></a> 2.4 Spam or ham?\n",
    "\n",
    "Let's illustrate that on the Ling-spam database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append('../1 - Text data preprocessing')\n",
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()\n",
    "print(\"data loaded\")\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded a training set with 2000 examples. This means that the Gaussian Process training will require inverting a $2000\\times2000$ matrix. Although this is still feasible in a few seconds or minutes, we are touching here a limitation of Gaussian Processes in their ability to scale up to large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "#from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "#spam_GP = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0))\n",
    "spam_GP = GaussianProcessClassifier()\n",
    "spam_GP.fit(Xtrain.toarray(),ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_GP.score(Xtest.toarray(),ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a fair comparison, we can also try the training on the word count features rather than Tf-Idf ones. Although it would probably be more beneficial to search for a better kernel than the default squared exponential one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')\n",
    "spam_GP = GaussianProcessClassifier()\n",
    "spam_GP.fit(Xtrain.toarray(),ytrain)\n",
    "spam_GP.score(Xtest.toarray(),ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep this last classifier and identify which are the misclassified emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')\n",
    "spam_GP = GaussianProcessClassifier()\n",
    "spam_GP.fit(Xtrain.toarray(),ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "ypredict = spam_GP.predict(Xtest.toarray())\n",
    "misclass = np.not_equal(ypredict, ytest)\n",
    "Xmisclass = Xtest[misclass,:]\n",
    "ymisclass = ytest[misclass]\n",
    "misclass_indices = [i for i, j in enumerate(misclass) if j == True]\n",
    "print(\"Misclassified messages indices:\", misclass_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(ytest, ypredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some misclassified mails\n",
    "index = misclass_indices[0]+2000\n",
    "print(\"Prediction:\", spam_GP.predict(spam_data.word_count[index,:].toarray()))\n",
    "spam_data.print_email(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec2-5\"></a> 2.5 NIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "print(digits.data.shape)\n",
    "print(digits.images.shape)\n",
    "print(digits.target.shape)\n",
    "print(digits.target_names)\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "Xtrain,Xtest = np.split(X,[1000])\n",
    "ytrain,ytest = np.split(y,[1000])\n",
    "#Xtrain = X[:1000,:]\n",
    "#ytrain = y[:1000]\n",
    "#Xtest = X[1000:,:]\n",
    "#ytest = y[1000:]\n",
    "\n",
    "#plt.gray();\n",
    "#plt.matshow(digits.images[0]);\n",
    "#plt.show();\n",
    "#plt.matshow(digits.images[15]);\n",
    "#plt.show();\n",
    "#plt.matshow(digits.images[42]);\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def shuffle_and_split(X,y,n):\n",
    "    X0,y0 = shuffle(X,y)\n",
    "    Xtrain,Xtest = np.split(X0,[n])\n",
    "    ytrain,ytest = np.split(y0,[n])\n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "digits_gp = GaussianProcessClassifier(n_restarts_optimizer=9)\n",
    "digits_gp.fit(Xtrain,ytrain)\n",
    "prediction = digits_gp.predict(Xtest)\n",
    "#print(\"Training error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest))\n",
    "print(\"Generalization error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest) )\n",
    "print(\"Generalization score:\", digits_gp.score(Xtest,ytest))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(ytest, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect illustration that (often) ML methods don't work out-of-the-box and require some engineering knowledge just like any other modeling method. Here for example, a data scientist might investigate the kernel's properties, the marginal likelihood optimization procedure, the data distribution..., while the computer vision expert might question the use of pixel data as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

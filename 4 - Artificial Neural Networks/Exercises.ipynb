{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f58ef7a",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/deep-learning/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778ca9c",
   "metadata": {},
   "source": [
    "# Variations on neural networks and stochastic gradient descent\n",
    "\n",
    "These exercises are meant to help you play around with the objects we have seen in class. They should help you get a more abstract and refined understanding of the mechanisms behind neural networks. Some are easy, others are harder and might require some thinking: this is intended to push you to consider different fascets of ANNs. Some answers are voluntarily very short to encourage discussion, either among students or with teaching assistants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858bb747",
   "metadata": {},
   "source": [
    "## From neural networks to computation graphs, and back\n",
    "\n",
    "We have seen that a feed-forward neural network is a computation graph, organized in layers. Let $f(\\cdot, \\Theta)$ be a fully connected neural network parameterized by $\\Theta$, made of $L$ layers, with $\\theta_l$ the weights and biases of layer $l$. \n",
    "Let $\\sigma_l$ be a Lipschitz continuous activation function. \n",
    "Then, the forward pass can be written:\n",
    "\n",
    "\\begin{align}\n",
    "    x_0 &= [x \\ 1]  \\\\\n",
    "    z_l &= \\theta_l \\cdot x_{l-1} \\\\\n",
    "    x_l &= \\sigma_l\\left(z_l\\right) \\\\\n",
    "    f(x, \\Theta) &= x_L \n",
    "\\end{align}\n",
    "\n",
    "The notation $[x \\ 1]$ for $x_0$ indicates that we append a constant \"1\" component to $x$. Similarly, when $\\sigma_l$ includes a constant \"1\" component, the notation above permits seamless integration of the biases within $\\theta_l$.\n",
    "\n",
    "Let $d_l$ denote the number of neurons on layer $l$, including the \"1\" constant ($d_0-1$ is hence the number of input variables).\n",
    "\n",
    "Note that $x$ is a matrix of dimension $(d_0-1)\\times B$ where $B$ is the minibatch size.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "What is the dimension of $\\theta_l$?\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "$$\\theta_l \\in \\mathbb{R}^{d_l \\times d_{l-1}}$$\n",
    "</details>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Write the network's output $f(x;\\Theta)$ as a composition of functions and matrix-vector products, using the $\\sigma_l$ and $\\theta_l$ notations.\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "$$f(x;\\Theta) = \\sigma_L\\left( \\theta_L \\cdot \\sigma_{L-1} \\left( \\theta_{L-1} \\cdot \\sigma_{L-1} \\left( \\ldots \\theta_0 \\cdot x_0 \\right) \\right) \\right)$$\n",
    "</details>\n",
    "\n",
    "Let $\\ell(z,z')$ be the loss used to fit the neural network. \n",
    "Computation of the empirical risk $\\mathcal{L}$ for a given minibatch is yet another computation graph.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "How is this graph related to that of $f(x, \\Theta)$?\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "Overall, this graph is a function of $\\Theta$, $x$ and $y$.\n",
    "The graph of $f(x, \\Theta)$ is a subgraph of that of $\\mathcal{L}(x,y,\\Theta)$, since:\n",
    "$$\\mathcal{L}(x,y,\\Theta) = \\sum_{x,y} \\ell\\left( f(x, \\Theta), y \\right)$$\n",
    "</details>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Draw inspiration from the derivation in class of the backpropagation algorithm with a mean square error, to derive a generic formulation for backpropagation.\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "The backward pass, recursively computes the $\\nabla_{\\theta_l} \\mathcal{L} (\\Psi(x, \\Theta); y)$ gradients for each layer $l$, in order to update the network parameters as:\n",
    "\n",
    "\\begin{align}\n",
    "    \\delta_L &= \\sigma_L'(z_L), \\\\\n",
    "    \\theta_{l} &\\leftarrow \\theta_l - \\eta \\left( \\delta_l \\cdot x_{l-1}^T \\right) \\nabla_{x_L} \\mathcal{L},  \\\\\n",
    "    \\delta_{l-1} &= \\sigma_{l-1}'(z_{l-1}) \\circ \\left( \\delta_l \\cdot \\theta_{l-1}  \\right), \n",
    "\\end{align}\n",
    "where $\\circ$ is the Hadamard product. \n",
    "</details>\n",
    "\n",
    "Overall, all that we have done in this example, is define a general computation graph $\\mathcal{L}(x,y,\\Theta)$, which is a composition of successive functions, and compute it's gradient $\\nabla_\\Theta \\mathcal{L}(x,y,\\Theta)$ by invoking the chain rule. It is important to note that the application of the chain rule is not specific to neural networks: it is applicable to any feedforward computational graph for which the $\\sigma_l$ are differentiable. This more general context is called **[automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)**. The backpropagation algorithm is a specific case of reverse mode automatic differentiation. If one considers computer programs which are composed of differentiable atomic building blocks, one defines the field of **[differentiable programming](https://en.wikipedia.org/wiki/Differentiable_programming)**. Modern neural network libraries are primarily automatic differentiation libraries, with an specific API for the application to neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae6f24",
   "metadata": {},
   "source": [
    "## Computational cost of the forward and backward passes\n",
    "\n",
    "We have seen that a feed-forward neural network is a computation graph, organized in layers. Let $f(\\cdot, \\Theta)$ be a fully connected neural network parameterized by $\\Theta$, made of $L$ layers, with $\\theta_l$ the weights and biases of layer $l$. \n",
    "Let $\\sigma_l$ be a Lipschitz continuous activation function. \n",
    "Then, the forward pass can be written:\n",
    "\n",
    "\\begin{align}\n",
    "    x_0 &= [x \\ 1]  \\\\\n",
    "    z_l &= \\theta_l \\cdot x_{l-1}\\\\\n",
    "    x_l &= \\sigma_l\\left(z_l\\right)\\\\\n",
    "    f(x, \\Theta) &= x_L \n",
    "\\end{align}\n",
    "\n",
    "The notation $[x \\ 1]$ for $x_0$ indicates that we append a constant \"1\" component to $x$. Similarly, when $\\sigma_l$ includes a constant \"1\" component, the notation above permits seamless integration of the biases within $\\theta_l$.\n",
    "\n",
    "For the sake of simplicity, let $n$ be the number of neurons on each hidden layer, let $B$ be the mini-batch size and let $x_L$ have dimension one.\n",
    "\n",
    "Let $\\mathcal{L}$ be the empirical risk used to fit the neural network. \n",
    "The backward pass, recursively computes the $\\nabla_{\\theta_l} \\mathcal{L} (\\Psi(x, \\Theta); y)$ gradients for each layer $l$, in order to update the network parameters as:\n",
    "\n",
    "\\begin{align}\n",
    "    \\delta_L &= \\sigma_L'(z_L), \\\\\n",
    "    \\theta_{l} &\\leftarrow \\theta_l - \\eta \\left( \\delta_l \\cdot x_{l-1}^T \\right) \\nabla_{x_L} \\mathcal{L},  \\\\\n",
    "    \\delta_{l-1} &= \\sigma_{l-1}'(z_{l-1}) \\circ \\left( \\delta_l \\cdot \\theta_{l-1}  \\right), \n",
    "\\end{align}\n",
    "where $\\circ$ is the Hadamard product. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "What is the time complexity of the forward pass?\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "$z_l = \\theta_l \\cdot x_{l-1}$ induces $O(n^2B)$ operations.  \n",
    "$x_l = \\sigma_l\\left(z_l\\right)$, in turn, induces $O(nB)$ operations.  \n",
    "Overall, the forward pass induces $O(Ln^2B)$ operations.\n",
    "</details>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "What is the time complexity of the backward pass?\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "$\\theta_{l} \\leftarrow \\theta_l - \\eta \\left( \\delta_l \\cdot x_{l-1}^T \\right) \\nabla_{x_L} \\mathcal{L}$ induces $O(n^2B)$ operations.  \n",
    "$\\delta_{l-1} = \\sigma_{l-1}'(z_{l-1}) \\circ \\left( \\delta_l \\cdot \\theta_{l-1}  \\right)$ induces $O((n + n^2)B)$ operations.  \n",
    "Hence the backward pass induces $O(2Ln^2B)$ operations.\n",
    "</details>\n",
    "\n",
    "The point of this simple reminder is to recall that the complexity classes of both operations are overall the same, but that the backward pass requires (an order of) twice as many operations as the forward pass. \n",
    "\n",
    "Of course, parallelization and caching in modern computation architectures may greatly amortize this complexity and make it unnoticeable in some cases.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Verify this experimentally using the code developped in class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf65f823",
   "metadata": {},
   "source": [
    "## Convergence speed of SGD\n",
    "\n",
    "Consider a stochastic gradient descent process, minimizing a regression loss for function $f_\\theta$, currently at step $k$. \n",
    "Let $\\theta^*$ denote the optimal solution to the optimization problem.\n",
    "The current parameter is $\\theta_k$.\n",
    "A mini-batch has been drawn from the training set and yielded a gradient estimate $g_k$. \n",
    "\n",
    "Given a gradient step $\\eta$, one has $\\theta_{k+1} = \\theta_k - \\eta g_k$.\n",
    "\n",
    "Let us define the convergence speed $S_k$ of stochastic gradient descent as:\n",
    "$$S_k = -\\mathbb{E}_{\\textrm{minibatch} \\sim \\bar{p}} \\left[ \\|\\theta_{k+1} - \\theta^*\\|_2^2 - \\|\\theta_k - \\theta^*\\|_2^2 \\right].$$\n",
    "\n",
    "Here, the expectation is taken with respect to the minibatch drawn, $\\bar{p}$ being the empirical distribution of the training set. \n",
    "This quantity measures how much closer to $\\theta^*$ we can expect $\\theta_{k+1}$ to be, compared to $\\theta_k$, on average when drawing mini-batches.\n",
    "\n",
    "Let us drop the $k$ index and write \n",
    "$$S = -\\mathbb{E}_{\\textrm{minibatch} \\sim \\bar{p}} \\left[ \\|\\theta - \\eta g - \\theta^*\\|_2^2 - \\|\\theta - \\theta^*\\|_2^2 \\right].$$\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Write $S$ as an expression depending of $g$. There are two parts in this expression, what does each represent? In particular, how is one of the two terms connected to the distribution of losses values across the training set, at the current $\\theta$?\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\\begin{align}\n",
    "S &= -\\mathbb{E} \\left[ \\|\\theta - \\eta g - \\theta^*\\|_2^2 - \\|\\theta - \\theta^*\\|_2^2 \\right]\\\\\n",
    "&= -\\mathbb{E} \\left[ \\eta^2 g^T g - 2(\\theta-\\theta^*)^T g \\right]\\\\\n",
    "&= 2 (\\theta-\\theta^*)^T \\mathbb{E} \\left[ g \\right] -\\mathbb{E} \\left[ g^T g \\right]\n",
    "\\end{align}\n",
    "\n",
    "The first term is $\\theta$-dependent and relies on the expected gradient across minibatches. This expected gradient is always the same, whatever the batch size or the batch sampling method. So this term measures really how much we can gain on average for the considered function in the specific current $\\theta$.\n",
    "\n",
    "The second term however is the expected value, across mini-batches, of the square of the gradient norm $\\mathbb{E} \\left[ \\| g \\|^2_2\\right]$.\n",
    "If $\\theta$ was one-dimensional, then so would be the gradient. And in this case, $\\mathbb{E} \\left[g^2\\right]$ would be $Var(g) + \\mathbb{E}[g]^2$. Specifically, this second term would indicate the variance of the gradient norm. If the gradient norm had large variance across mini-batches, the convergence speed would be bad.\n",
    "\n",
    "Now $\\theta$ is multi-dimensional and the relationship is a bit more complex with $\\mathbb{E} \\left[ \\| g \\|^2_2\\right] = \\textrm{Tr}( \\mathbb{V} [g]) + \\mathbb{E}[g]^T \\mathbb{E} [g]$, with $\\mathbb{V} [g]$ the covariance matrix of the elements of $g$. \n",
    "Despite this somehow more convoluted expression, the interpretation is the same: this second term indicate a notion of variance of the gradient norm. \n",
    "Again, if the gradient norm has large variance across mini-batches, the convergence speed is smaller.\n",
    "\n",
    "When does that happen? This happens for instance when training errors are quite unbalanced across the training set. Suppose that for the current $\\theta$, we fit perfectly every single $(x,y)$ in the training set, but one which we write $\\tilde{x},\\tilde{y}$ for which we make an arbitrarily large error. Specifically, write $\\tilde{f} = f_\\theta(\\tilde{x}$, then we suppose $\\nabla_\\tilde{f} \\ell(\\tilde{f},\\tilde{y})$ is arbitrarily large. Then, if this sample is not picked in the minibatch, the gradient is exactly zero. But if $\\tilde{x},\\tilde{y}$ is picked, then the gradient is non-zero and its norm is proportional to $\\nabla_\\tilde{f} \\ell(\\tilde{f},\\tilde{y})$. These discrepancies induce a large variability in the gradient's norm, which penalizes the convergence speed.\n",
    "</details>\n",
    "\n",
    "This short exercise was inpired by these two papers:  \n",
    "Wang, L., Yang, Y., Min, R., & Chakradhar, S. (2017). [Accelerating deep neural network training with inconsistent stochastic gradient descent](https://arxiv.org/abs/1603.05544). Neural Networks, 93, 219-229.   \n",
    "Katharopoulos, A., & Fleuret, F. (2018). [Not all samples are created equal: Deep learning with importance sampling](https://arxiv.org/abs/1803.00942). In International conference on machine learning.\n",
    "\n",
    "In particular, the second paper notes that the second term we found above can be increased by sampling mini-batches non uniformly in the training set. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Read the 3 first pages of the paper (until the end of Section 3.2) and implement their method using the code developped in class (discard Section 3.3 to keep things relatively simple).\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Backpropagation\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| Emmanuel Rachelson and Dennis G. Wilson | <a href=\"https://supaerodatascience.github.io/deep-learning/\">https://supaerodatascience.github.io/deep-learning/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ANN, value propagation (forward pass), and error backpropagation (backward pass), code from the main notebook are all available in `ann.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T09:02:11.815491Z",
     "start_time": "2019-11-21T09:02:11.698167Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this code to visualize value propagation and error backpropagation during training using the same neural network architecture as seen in class.\n",
    "\n",
    "<img src=\"img/nn3.png\" width=\"600px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T09:02:11.820414Z",
     "start_time": "2019-11-21T09:02:11.817597Z"
    }
   },
   "outputs": [],
   "source": [
    "sizes = [2,4,3,1]\n",
    "myann = ann.ANN(sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the network, we'll use the `networkx` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.append([0], np.cumsum(sizes))\n",
    "id_layers = []\n",
    "G = nx.Graph()\n",
    "for l in range(1, len(ids)):\n",
    "    id_layers.append([i for i in range(ids[l-1], ids[l])])\n",
    "for l in range(len(id_layers)):\n",
    "    for i in id_layers[l]:\n",
    "        G.add_node(i, subset=l)\n",
    "for l in range(1, len(id_layers)):\n",
    "    for i in id_layers[l-1]:\n",
    "        for j in id_layers[l]:\n",
    "            G.add_edge(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [i for l in myann.weights for i in l.flatten()]\n",
    "biases = np.concatenate((np.zeros(sizes[0]),\n",
    "                         [i for l in myann.biases for i in l.flatten()]))\n",
    "first_weights = np.copy(weights)\n",
    "first_biases = np.copy(biases)\n",
    "options = {\n",
    "    \"width\": 4,\n",
    "    \"cmap\": plt.cm.Reds,\n",
    "    \"edge_cmap\": plt.cm.Blues,\n",
    "    \"with_labels\": True,\n",
    "}\n",
    "pos = nx.multipartite_layout(G)            \n",
    "nx.draw(G, pos, **options, edge_color=weights, node_color=biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll train the network to approximate the function $x_0,x_1\\mapsto \\sqrt{x_0+x_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T09:02:11.897842Z",
     "start_time": "2019-11-21T09:02:11.822149Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# generate training and testing sets\n",
    "def func(x):\n",
    "    return np.sqrt(x[:,0]+x[:,1])\n",
    "\n",
    "testing_x  = np.random.uniform(size=(100,2))\n",
    "testing_y  = func(testing_x).reshape(-1,1)\n",
    "training_x = np.random.uniform(size=(100,2))\n",
    "training_y = func(training_x).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, s, z = myann.forward_pass(training_x, verbose=False)\n",
    "out = training_y\n",
    "error = z[-1] - out\n",
    "plt.scatter(training_x[:, 0], training_y, label='target')\n",
    "plt.scatter(training_x[:, 0], error, label='error')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 24))\n",
    "# initialize gradient vectors for each layer\n",
    "delta = [np.zeros((out.shape[0], sz)) for sz in myann.layer_sizes]\n",
    "grad_w = [np.zeros((o, i)) for i, o in\n",
    "      zip(myann.layer_sizes[:-1], myann.layer_sizes[1:])]\n",
    "grad_b = [np.zeros((1, y)) for y in myann.layer_sizes[1:]]\n",
    "#learning rate\n",
    "alpha = 0.01\n",
    "for i in range(len(myann.layer_sizes)-1, 0, -1):\n",
    "    if i == len(myann.layer_sizes)-1:\n",
    "        delta[i] = s[-1]\n",
    "    else:\n",
    "        delta[i] = np.dot(delta[i+1], myann.weights[i])\n",
    "        delta[i] = np.multiply(delta[i], s[i])\n",
    "    # delta at each layer\n",
    "    delta_temp = np.multiply(delta[i], error)\n",
    "    # reset weight gradients to 0 for plotting\n",
    "    grad_w = [np.zeros((o, i)) for i, o in\n",
    "          zip(myann.layer_sizes[:-1], myann.layer_sizes[1:])]\n",
    "    # gradient of weights at this layer\n",
    "    grad_w[i-1] = np.dot(delta_temp.T, z[i-1])\n",
    "    # reset bias gradients to 0 for plotting\n",
    "    grad_b = [np.zeros((1, y)) for y in myann.layer_sizes[1:]]\n",
    "    # gradients of biases at this layer\n",
    "    grad_b[i-1] = np.sum(delta_temp, axis=0)\n",
    "    # flatten weight and bias gradients for plotting\n",
    "    dw_flat = [np.abs(i) for l in grad_w for i in l.flatten()]\n",
    "    db_flat = np.concatenate((np.zeros(sizes[0]),\n",
    "                              [np.abs(i) for l in grad_b for i in l.flatten()]))\n",
    "    # plot\n",
    "    plt.subplot(len(myann.layer_sizes), 1, len(myann.layer_sizes)-i)\n",
    "    plt.title(\"Gradients for layer \"+str(i))\n",
    "    nx.draw(G, pos, **options, edge_color=dw_flat, node_color=db_flat)\n",
    "    # IMPORTANT: update weights for next step of backprop\n",
    "    myann.weights[i-1] -= alpha * grad_w[i-1]\n",
    "    myann.biases[i-1] -= alpha * grad_b[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Before update\")\n",
    "nx.draw(G, pos, **options, edge_color=first_weights, node_color=first_biases)\n",
    "weights = [i for l in myann.weights for i in l.flatten()]\n",
    "biases = np.concatenate((np.zeros(sizes[0]),\n",
    "                         [i for l in myann.biases for i in l.flatten()]))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"After update\")\n",
    "nx.draw(G, pos, **options, edge_color=weights, node_color=biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T09:02:11.991851Z",
     "start_time": "2019-11-21T09:02:11.899623Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute initial generalization loss\n",
    "y,s,z = myann.forward_pass(testing_x, verbose=False)\n",
    "err = np.mean((z[-1]-testing_y)**2)\n",
    "print(\"Generalization loss estimate:\",err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one backwards step. For training, we'll use the definition of `backward_pass` in the `ANN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T09:02:12.357980Z",
     "start_time": "2019-11-21T09:02:11.995306Z"
    }
   },
   "outputs": [],
   "source": [
    "# myann.reset_weights()\n",
    "nsteps = 1000\n",
    "training_loss = np.zeros(nsteps)\n",
    "testing_loss  = np.zeros(nsteps)\n",
    "\n",
    "for i in range(nsteps):\n",
    "    _,_,z_test = myann.forward_pass(testing_x, verbose=False)\n",
    "    testing_loss[i]  = np.mean((z_test[-1]-testing_y)**2)\n",
    "    y_train,s_train,z_train = myann.forward_pass(training_x, verbose=False)\n",
    "    training_loss[i] = np.mean((z_train[-1]-training_y)**2)\n",
    "    myann.backward_pass(training_y, y_train, s_train, z_train, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T09:02:13.156148Z",
     "start_time": "2019-11-21T09:02:12.360321Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(training_loss, c='b', label='training')\n",
    "plt.semilogy(testing_loss, c='r', label='validation')\n",
    "plt.legend()\n",
    "print(\"last training loss:\", training_loss[-1])\n",
    "print(\"last generalization loss:\", testing_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

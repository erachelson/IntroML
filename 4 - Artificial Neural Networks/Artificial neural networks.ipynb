{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/deep-learning/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Artificial Neural Networks</div>\n",
    "\n",
    "1. [Biological neural networks](#sec1)\n",
    "2. [Why invent artificial neural networks?](#sec2)\n",
    "2. [Artificial neural networks](#sec3)\n",
    "3. [Propagating values through a network](#sec4)\n",
    "3. [Learning the weights of a neural network (regression case)](#sec5)\n",
    "4. [Neural networks for classification](#sec6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id=\"sec1\"></a> Biological neural networks\n",
    "\n",
    "In 1839, *Cell theory* introduces the idea that living bodies are made of cells.<br>\n",
    "**Mikroskopische Untersuchungen über die Uebereinstimmung in der Struktur und dem Wachsthum der Thiere und Pflanzen.**<br>\n",
    "Schwann, Theodor. Berlin: Sander. (1839).\n",
    "\n",
    "But due to limitations in microsopy in the XIXth century, no one had observed the basic constituents of nerve tissue and the nervous system stood as an exception to cell theory.\n",
    "\n",
    "In the first issue of the Revista Trimestral de Histología Normal y Patológica (May, 1888), Santiago Ramón y Cajal shows a physical separation between individual cells at the axon/dendrite connection (in chickens). This lays the basis of the **neuron doctrine** which lead to his 1906 Nobel prize for Physiology or Medecine.<br>\n",
    "**Neuron theory, the cornerstone of neuroscience, on the centenary of the Nobel Prize award to Santiago Ramón y Cajal**.<br>\n",
    "López-Muñoz, F., Boya, J., & Alamo, C. Brain research bulletin, 70(4-6), 391-405. (2006).\n",
    "\n",
    "Neuron doctrine:\n",
    "- neurons are the basis constituent of the nervous system\n",
    "- dendrites $\\rightarrow$ nucleus $\\rightarrow$ axon $\\rightarrow$ synapses (Law of Dynamic Polarization).\n",
    "- electrical impulses.\n",
    "- chemical neuro-transmitters.\n",
    "\n",
    "<img src=\"img/neuron_bio.png\" width=\"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id=\"sec2\"></a> Why invent \"artificial neural networks\"?\n",
    "\n",
    "Our rationale goes as follows.<br>\n",
    "The human brain processes information efficiently, can we design an artificial computing method that mimics it?<br>\n",
    "But computers process bits, so our artificial neurons should take logical values as inputs.\n",
    "\n",
    "Let's take inspiration from the biological neuron for that and make a simplified model of a neuron. Suppose an input signal in the form of a binary vector $x$. The elements of $x$ can indicate binary statements which are true or false, such as \"it's raining\" or \"I have an umbrella\". We will model the *activation* of a neuron as:\n",
    "$$f(x)=\\left\\{\\begin{array}{ll} 0 & \\textrm{if }w^T x+b\\leq 0 \\\\ 1 & \\textrm{otherwise}\\end{array}\\right.=step\\left(w^Tx + b\\right),$$\n",
    "where $step$ is Heaviside's step function.\n",
    "\n",
    "We will call such a function *Rosenblatt's Perceptron*.\n",
    "Basically, a perceptron is a linear separation rule.\n",
    "Intuitively, it is a machine that weights evidence $x$ and compares it to threshold $b$ in order to make a decision $f(x)$.\n",
    "\n",
    "Although perceptrons were invented in the 50's and are not really representative of modern artificial neural networks, manipulating them conveys some of the important intuitions about artificial networks, so we will go into a \"back to the future\" mode for the next paragraphs before going any further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice: Back to the future, logical gates.** <br>\n",
    "Take Rosenblatt's perceptron and find input weights that correspond to AND, OR and NAND gates (for two binary inputs).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Rosenblatt's perceptron is the function $step(w_1 x_1 + w_2 x_2 +b)$. \n",
    "\n",
    "With $(w_1, w_2, b) = (2,2,-1)$ we get an OR gate. \n",
    "\n",
    "With $(w_1, w_2, b) = (2,2, -3)$ we get an AND gate. \n",
    "\n",
    "With $(w_1, w_2, b) = (-2,-2,3)$ we get a NAND gate.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perceptron(w,b,x):\n",
    "    return np.heaviside(np.dot(x,w)+b,0)\n",
    "\n",
    "x = np.zeros((4,2))\n",
    "x[1,0] = 1.\n",
    "x[2,1] = 1.\n",
    "x[3,0] = 1.\n",
    "x[3,1] = 1.\n",
    "\n",
    "print(\"input values:\\n\", x)\n",
    "\n",
    "def OR(x):\n",
    "    w = np.array([2.,2.])\n",
    "    b = -1.\n",
    "    return perceptron(w,b,x)\n",
    "print(\"testing OR gate:\", OR(x))\n",
    "\n",
    "def AND(x):\n",
    "    w = np.array([2.,2.])\n",
    "    b = -3.\n",
    "    return perceptron(w,b,x)\n",
    "print(\"testing AND gate:\", AND(x))\n",
    "\n",
    "def NAND(x):\n",
    "    w = np.array([-2.,-2.])\n",
    "    b = 3.\n",
    "    return perceptron(w,b,x)\n",
    "print(\"testing NAND gate:\", NAND(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice: Back to the future, the XOR affair.** <br> \n",
    "Does it seem possible to describe a XOR gate with a perceptron? Building a XOR function can be seen as a classification problem; what is the family of classification problems that can be tackled by perpectrons (hint: recall the beginning of the SVM class)?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "No, it is actually not possible to model a XOR gate with a perceptron. Since perceptrons implement a threshold on a linear combination of the inputs, they can only separate (shatter, in VC theory) classes that are... linearly separable. XOR is a typical example of non-linearly separable data.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter([0,1],[0,1],c='b')\n",
    "plt.scatter([1,0],[0,1],c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Back to the future, networks of perceptrons.**<br> \n",
    "It is possible to connect perceptrons together to get a XOR function (for example by remarking that $x_1$ XOR $x_2$ = $(x_1$ OR $x_2)$ AND $(x_1$ NAND $x_2)$). It is actually possible to do so for any logical function. Such connected architectures are called Multi-Layer Perceptrons (MLP). This term was later used (abusively) for multi-layered networks of artifical neurons, regardless of their activation functions.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/xor.png\" width=\"600px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR(x):\n",
    "    y1 = OR(x)\n",
    "    y2 = NAND(x)\n",
    "    y = np.array([y1,y2]).T\n",
    "    return AND(y)\n",
    "print(\"input values:\\n\", x)\n",
    "print(\"testing XOR gate:\", XOR(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a consequence, any logical circuit can be represented as an MLP. **This seems a great ground for computer-based intelligence!**\n",
    "\n",
    "Now the question is \"how does one find (learn) the structure and weights of a neural network that seems intelligent?\". We will come to that in a minute; let's first play around a bit with artificial neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice: Weight invariance of the Heaviside neuron.**<br>\n",
    "If one multiplies all weights and the bias of a perceptron by a constant $c>0$, does the logical function change?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "A perceptron is invariant by scalar multiplication.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have considered a drastic \"0 or 1\" activation for a certain neuron. Suppose now that the input $x$ is not binary anymore: it is made of continuous variables, like a temperature, or a user preference. Then, when processing an input $x$, either the neuron's stimulation $w^Tx$ is above $-b$ or it is below. This makes the output of a neuron very sensitive to noise in the input, or to errors in setting the weights. Conversely, we could wish for a function that is *S-shaped* and that transitions smoothly from 0 to 1.\n",
    "\n",
    "An example of such a function is the sigmoid function:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1. + np.exp(-x))\n",
    "\n",
    "XX = np.arange(-10.,10.,0.1)\n",
    "plt.plot(XX,sigmoid(XX));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice: Weight invariance of the sigmoid neuron.**<br>\n",
    "Explain why the weight invariance property does not hold for sigmoid activation functions? Can we regain this invariance property when $c\\rightarrow\\infty$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "This weight invariance property is not true for a sigmoid neuron because, for a given input $x$, multiplying all input weights by $c$ boils down to a shift by a multiplicative factor of $c$ on the horizontal scale of the sigmoid function above, which changes the value taken on the vertical axis. However, when $c\\rightarrow\\infty$, the sigmoid acts as a $step(w^Tx+b)$ function so tends to the same behavior as the perceptron.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Beyond perceptrons.** Modern ANNs model logical functions as well as classification or regression tasks. For this purpose, one needs to find the correct weights so that the network actually produces the desired relation between inputs and outputs. Automatically tuning these weights from the data is the learning algorithm we need to design.\n",
    "</div>\n",
    "\n",
    "Note that the definition above restricts learning algorithms to finding optimized weights for a predefined set of neurons and connections. This is too reductive: finding the appropriate network structure is just as important. Unfortunately, Neural Architecture Search is still a pretty open research topic and, in this class, we will focus on weight learning for predefined network topologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, our progression of ideas has been:\n",
    "- The human brain processes information efficiently, can we design an artificial computing method that mimics it?\n",
    "- The biologicial neuron.\n",
    "- Rosenblatt's perceptron: $f(x)=step(\\sum w^T x+b)$ for a binary $x$ input vector.\n",
    "- Networks of artificial neurons = Artifical Neural Networks (ANN).\n",
    "- Generalization on activation functions and the particular case of the (logistic) sigmoid function.\n",
    "\n",
    "If you get all the ideas behind each of these steps, we can move on to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id=\"sec3\"></a>Artificial neural networks\n",
    "\n",
    "**The idea:**\n",
    "1. Each neuron processes a bit of information and passes it to its children.\n",
    "2. Overall the network processes raw information into general concepts.\n",
    "\n",
    "**Question:** can we mimic this system of connections in a learning system that adapts its parameters to the data it is exposed to?\n",
    "\n",
    "<img src=\"img/neuron_bio.png\" width=\"600px\"></img>\n",
    "\n",
    "**A formal neuron:**\n",
    "$$z = \\sigma(w^T x + b)$$\n",
    "- $x$ is the input (the $n$-dimensional signal received through dendrites)\n",
    "- $w$ is an $n$-dimensional vector of weights that give more or less importance to the elements of $x$\n",
    "- $b$ is a scalar bias\n",
    "- $\\sigma$ is the neuron's activation function\n",
    "- $z$ is the output (the signal along the \"axon\")\n",
    "\n",
    "Note that the formal neuron is a *function*, very different from the impulses carried by axons in biological neurons.\n",
    "\n",
    "<img src=\"img/artificial-neuron.png\" width=\"250px\">\n",
    "\n",
    "**Computation graph**\n",
    "\n",
    "A neural network is obtained by connecting some neuron's outputs to other neurons inputs. The goal of such a network is typically to learn how to imitate a certain function $f(x)$ for which we are given training data pairs $(x,y)$ with $y = f(x) + \\textrm{noise}$. Such a network has thus three types of neurons:\n",
    "- Input neurons. Those correspond to the different input variables $x_j$ describing our training examples.\n",
    "- Output neurons. Those correspond to the targets $y$ in our examples.\n",
    "- Hidden neurons. Any neuron that's not an input or an output neuron.\n",
    "\n",
    "Therefore, a neural network is a computation graph, with inputs $x$ and outputs $y$, where nodes are neurons and edges connect the output signal of a node to one of the inputs of another.\n",
    "\n",
    "**A little vocabulary:**\n",
    "- A neural network is a computation graph.\n",
    "- The input layer is composed of all input neurons.\n",
    "- A layer is a (maximum) set of unconnected neurons, at the same depth from the input layer.\n",
    "- The output layer is composed of all output neurons.\n",
    "- All layers between the input and output layers are called hidden layers.\n",
    "- A neural network organized in layers is called a feedforward NN.\n",
    "- Some neural networks are not feedforward NNs and present loops. They are called Recurrent NN.\n",
    "- A multilayer NN is often called a multilayer perceptron (for historical reasons)\n",
    "- The scalar output of a neuron is also called its activation (depending on sources, sometimes it is rather the scalar input that is called activation).\n",
    "- The vector of outputs for all neurons in a given layer is called the layer's activation.\n",
    "\n",
    "<img src=\"img/nn.png\" width=\"600px\"></img>\n",
    "\n",
    "**A bit of history:**<br>\n",
    "1943: McCulloch (neurophysiologist) and Pitts (logician) suggest a first formal model for neurons.\n",
    "> A logical calculus of the ideas immanent in nervous activity. McCulloch, W. and Pitts, W. Bulletin of Mathematical Biophysics, 5:115–133. (1943). [paper](https://link.springer.com/article/10.1007%2FBF02478259), [wikipedia](https://en.wikipedia.org/wiki/Artificial_neuron).\n",
    "\n",
    "1949: Hebb suggests dendrites are strengthened whenever they are used.\n",
    "> The Organization of Behavior. Hebb, D.O. New York: Wiley & Sons. (1949). [book](https://pure.mpg.de/rest/items/item_2346268_3/component/file_2346267/content), [wikipedia](https://en.wikipedia.org/wiki/Hebbian_theory).\n",
    "\n",
    "1951: [Minsky](https://en.wikipedia.org/wiki/Marvin_Minsky) designs a network of artificial neurons.\n",
    "> [wikipedia](https://en.wikipedia.org/wiki/Stochastic_neural_analog_reinforcement_calculator)\n",
    "\n",
    "1958: Roseblatt designs the Perceptron, with step activation functions.\n",
    "> The perceptron: A probabilistic model for information storage and organization in the brain. Rosenblatt, F. Psychological Review, 65(6), 386-408. (1958). [wikipedia](https://en.wikipedia.org/wiki/Perceptron), [paper](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.588.3775)\n",
    "\n",
    "1962: Widrow and Hoff suggest the delta-rule for adapting a network weights to obtain the desired output.\n",
    "> [wikipedia](https://en.wikipedia.org/wiki/Least_mean_squares_filter)\n",
    "\n",
    "1970s and 80s: The \"quiet years\", first AI winter. Attention turns to other methods while computing resources slowly increase. The Lighthill report halts AI research in the UK.\n",
    "> Artificial Intelligence: A General Survey. James Lighthill. Artificial Intelligence: a paper symposium, UK Science Research Council. (1973). [report](http://www.chilton-computing.org.uk/inf/literature/reports/lighthill_report/p001.htm)\n",
    "\n",
    "1986: Rediscovery of the backpropagation algorithm (for multilayered perceptrons).\n",
    "> Learning representations by back-propagating errors. Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J. Nature. 323 (6088): 533–536. 1986. [paper](https://www.nature.com/articles/323533a0).\n",
    "\n",
    "1990s: not enough computing power, not enough data, not enough recognition (while other methods like SVMs achieve good results). Second AI winter.\n",
    "\n",
    "2000s: computing power + available data allow the training of deep multi-layered architectures (deep learning). Massive successes.\n",
    "\n",
    "<div class=\"alert alert alert-success\"> Feedforward neural networks are computational graphs where edges convey scalar values between neurons (nodes).<br>\n",
    "A neuron's output results from the application of the activation function $\\sigma$ to a linear combination of its inputs $z = \\sigma(w^T x + b)$.<br>\n",
    "The network parameters are all the neuron's input weights and biases. <br>\n",
    "A neural network is a function that transforms its inputs into outputs by value propagation in the network.<br>\n",
    "Learning a neural network consists in finding the $w$ and $b$ such that the network's output matches the function $f(x)$ that generated the data pairs $(x,y = f(x)\\textrm{+noise})$.\n",
    "</div>\n",
    "\n",
    "**Universal approximation theorem:**<br>\n",
    "If $\\sigma$ is \"S-shaped\", then with enough neurons, a single layer, feed-forward NN can approximate any continuous function to an arbitrary precision.<br>\n",
    "In other words, NN are universal approximators.<br>\n",
    "<a href=\"UniversalApproximationTheorem.pdf\">Let's take a formal look at this theorem.</a>\n",
    "\n",
    "**Activation functions**\n",
    "- step\n",
    "$$\\sigma(x) = 0 \\textrm{ if }x\\leq0\\textrm{, }1\\textrm{ otherwise}$$\n",
    "- linear\n",
    "$$\\sigma(x) = x$$\n",
    "- sigmoid or logistic (which we will consider by default for now)\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "- hyperbolic tangent\n",
    "$$\\sigma(x) = \\frac{e^{x} + e^{-x}}{e^{x} - e^{-x}}$$\n",
    "- radial basis function (useful in some specific cases like Kohonen maps)\n",
    "$$\\sigma(x) = e^{-x^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec4\"></a>4. Propagating values through a network\n",
    "\n",
    "Let's define a neural network that has the following structure:\n",
    "- 2 input neurons\n",
    "- first hidden layer with 4 sigmoid neurons\n",
    "- second hidden layer with 3 sigmoid neurons\n",
    "- one ouput layer with an identity neuron\n",
    "\n",
    "<img src=\"img/nn2.png\" width=\"600px\"></img>\n",
    "\n",
    "Let's initialize its weights randomly (following a $\\mathcal{N}(0,1)$ distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [2,4,3,1]\n",
    "num_layers = len(sizes)\n",
    "biases = [np.random.randn(1,y) for y in sizes[1:]]\n",
    "weights = [np.random.randn(out,inp) for inp,out in zip(sizes[:-1],sizes[1:])]\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**<br>\n",
    "Write a function that computes the forward propagation of the input $x=[1,2]$ through the network and returns the outputs and all intermediate activations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "input_value = np.array([[1,2]])\n",
    "\n",
    "def forward_pass(x, verbose=False):\n",
    "    z = [np.zeros((x.shape[0], sz)) for sz in sizes]\n",
    "    y = [np.zeros((x.shape[0], sz)) for sz in sizes]\n",
    "    z[0] = x.copy()\n",
    "    for i in range(1,len(sizes)):\n",
    "        if verbose:\n",
    "            print(\"# Forward propagation to layer\", i)\n",
    "        y[i] = np.dot(z[i-1],weights[i-1].T) + biases[i-1]\n",
    "        if verbose:\n",
    "            print(\"Neuron inputs:\", y[i])\n",
    "        if i==len(sizes)-1:\n",
    "            z[i] = y[i]\n",
    "        else:\n",
    "            z[i] = sigmoid(y[i])\n",
    "        if verbose:\n",
    "            print(\"Layer outputs:\", z[i])\n",
    "    return y,z\n",
    "\n",
    "y,z = forward_pass(input_value, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id=\"sec5\"></a> Learning the weights of a neural network (regression case)\n",
    "\n",
    "Suppose that the value associated to $x=[1,2]$ in the previous example was $12.3$. That is not what was predicted and we could wish to adapt the weights of the network so that the next time we propagate $x=[1,2]$ through the network, the prediction is closer to $12.3$.\n",
    "\n",
    "Let's do a bit of math to formalize the search for the best neural network for our data.\n",
    "\n",
    "Let's first suppose that our data points $(x,y)$ are drawn from a probability distribution $p(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Risk minimization and loss functions\n",
    "\n",
    "After all, a neural network with a fixed graph structure is a parametric function $f_\\theta$ where $\\theta$ is the vector of all parameters (all weights and biases). \n",
    "Learning a neural network that correctly predicts $y$ corresponds to finding the parameters $\\theta$ that minimize the following function.\n",
    "$$L(\\theta) = \\displaystyle \\mathbb{E}_{(x,y)\\sim p(x,y)} \\left[ \\left(f_\\theta(x) - y\\right)^2 \\right] = \\int_{x,y} \\left[ \\left(f_\\theta(x) - y\\right)^2 \\right] \\mathrm{d}p(x,y) $$\n",
    "\n",
    "The smaller $L(\\theta)$, the happier we are.\n",
    "\n",
    "Note that this defines a least squares minimization problem. Let's take a minute to generalize slightly before we turn back to this least squares minimization problem.\n",
    "\n",
    "Let $\\ell(\\hat{y},y)$ denote how unhappy we are if one predicts $\\hat{y}$ when $y$ was expected. $\\ell$ is referred to as a **cost** or **loss function**: it measures the cost of the joint event \"$\\hat{y},y$\". An example, when $\\hat{y}$ and $y \\in \\mathbb{R}$ is $\\ell(\\hat{y},y)=(\\hat{y}-y)^2$. But recall the class on support vector regression: there we used the $\\epsilon$-insensitive loss $\\ell(\\hat{y},y) = \\max\\{0,1-|\\hat{y}-y|\\}$. We also introduced a few other loss functions there (Laplacian, squared, Huber, etc.). Let's expand a bit and suppose our prediction is not a scalar value anymore, but rather the probability of belonging to one of $C$ classes. Then $y$ and $\\hat{y}$ are probability distributions over the $C$ classes, and their cross-entropy measures the discrepancy between them, defining the cross-entropy loss $\\ell(\\hat{y},y) = -\\sum_{i=1}^C y_i \\log \\hat{y}_i$. The point here is: a loss function measures the penalty we pay for predicting $\\hat{y}$ instead of $y$. \n",
    "\n",
    "Then, for a given prediction function $f$, one can define the expected value of the loss for the distribution $p(x,y)$: we shall call it the **expected loss** or the **risk** $L(f) = \\mathbb{E}_{(x,y)\\sim p(x,y)} \\left[ \\ell(f(x),y) \\right]$. For a parametrized function $f_\\theta$, we directly define $L(\\theta)$. Overall, a good predictor is one which **minimizes the risk**.\n",
    "\n",
    "**Risk minimization** hence consists in finding $f^* \\in \\arg\\min_{f \\in \\mathcal{F}} L(f)$, where $\\mathcal{F}$ is a certain family of functions. In the present case, since the structure of the network is fixed, this family of functions is the one spanned by varying the network parameters $\\theta$. But computing the risk is intractable in practice because $p(x,y)$ is unknown. However, in supervised learning, we rely on training sets $\\{(x_i,y_i)\\}_{i\\in [1,N]}$ whose elements have all been sampled independently of each other and according to $p(x,y)$. The training set defines the empirical measure $\\bar{p}(x,y)$ and we can approximate the risk by the **empirical risk**:\n",
    "$$\\bar{L}(f) = \\mathbb{E}_{(x,y)\\sim \\bar{p}(x,y)} \\left[ \\ell(f(x),y) \\right] = \\frac{1}{N} \\sum_{i=1}^N \\ell(f(x_i),y_i).$$\n",
    "\n",
    "The empirical risk is the Monte Carlo estimate of the risk, given the training set. Overall, this generalization is the core principle of **empirical risk minimization**, which itself is the foundation of neural network optimization.\n",
    "\n",
    "> **Principles of risk minimization for learning theory.** Vapnik, V. Advances in neural information processing systems. (1991) [paper](https://proceedings.neurips.cc/paper/1991/hash/ff4d5fbbafdf976cfdc032e3bde78de5-Abstract.html)\n",
    "\n",
    "Note that a common vocabulary shortcut in modern deep learning is to call $\\bar{L}$ the loss (while the loss really is $\\ell$). Generally this does not lead to any notable confusion in practice.\n",
    "\n",
    "But, for now, let's get back to the squared loss function, which gave rise to the least squares minimization problem above.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**<br>\n",
    "Using the training data provided below, estimate the empirical risk for the current parameters $\\theta$ of the network defined in the previous section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X = np.array([[1.,2.],[1.1,1.7],[0.8,1.9]])\n",
    "def func(x):\n",
    "    return np.sqrt(x[:,0]+x[:,1])\n",
    "output_y = func(input_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code2.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "ypred,zpred = forward_pass(input_X, verbose=True)\n",
    "pred = zpred[-1]\n",
    "err = np.mean((pred-output_y)**2)\n",
    "print(\"Empirical risk estimate:\",err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Stochastic gradient descent\n",
    "\n",
    "Let's say we have an initial guess $\\theta_0$ for the parameters of $f_\\theta$. How can we change this guess so that we minimize $L(\\theta)$? Plain gradient descent tells us we should move in the opposite direction of the gradient of $L(\\theta)$ with respect to $\\theta$. So let's write this gradient:\n",
    "\n",
    "\\begin{align*}\n",
    "\\displaystyle \\nabla_\\theta L(\\theta) &= \\nabla_\\theta \\left[ \\mathbb{E}_{(x,y)\\sim p(x,y)} \\left[ \\left(f_\\theta(x) - y\\right)^2 \\right] \\right]\\\\\n",
    "&= \\mathbb{E}_{(x,y)\\sim p(x,y)} \\left[ \\nabla_\\theta \\left[ \\left(f_\\theta(x) - y\\right)^2 \\right] \\right]\\\\\n",
    "&= \\mathbb{E}_{(x,y)\\sim p(x,y)} \\left[ 2 \\left(f_\\theta(x) - y\\right) \\nabla_\\theta f_\\theta(x) \\right]\n",
    "\\end{align*}\n",
    "\n",
    "So, the gradient of $L(\\theta)$ is the expectation of $2 \\left(f_\\theta(x) - y\\right) \\nabla_\\theta f_\\theta(x)$. In other words:\n",
    "\n",
    "$$\\nabla_\\theta L(\\theta) = \\int_{x,y} 2 \\left(f_\\theta(x) - y\\right) \\nabla_\\theta f_\\theta(x) \\mathrm{d}p(x,y)$$\n",
    "\n",
    "The problem with this expression is that it requires the knowledge of $p(x,y)$ for all possible $(x,y)$ pairs (just like for computing the risk). That would mean having an infinite amount of data. We can, however, try to substitute the risk by the empirical risk, and approximate this gradient with a finite data set $\\left\\{\\left(x_i,y_i\\right)\\right\\}_{i\\in [1,N]}$ drawn i.i.d. according to $p$:\n",
    "$$\\nabla_\\theta L(\\theta) \\approx \\nabla_\\theta \\bar{L}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N 2 \\left(f_\\theta(x_i) - y_i\\right) \\nabla_\\theta f_\\theta(x_i)$$\n",
    "\n",
    "This is actually a *noisy estimation of the gradient* (that converges to the true gradient in the limit of infinite sampling).\n",
    "To be precise, it is a Monte Carlo estimator of the expectation in $\\nabla_\\theta L(\\theta)$. \n",
    "The theory of *stochastic gradient descent* tells us that if $g(\\theta)$ is a noisy estimator of $\\nabla_\\theta L(\\theta)$, then the following sequence $\\theta_k$ converges to a local minimum of $L(\\theta)$:\n",
    "$$\\theta_{k+1} = \\theta_k - \\alpha_k g(\\theta_k)$$\n",
    "under the condition that $\\sum \\alpha_k = \\infty$ and $\\sum \\alpha_k^2 < \\infty$ (called the Robbins-Monro conditions).\n",
    "\n",
    "> **A Stochastic Approximation Method**.\n",
    "H. Robbins and S. Monro (1951). *The Annals of Mathematical Statistics*. **22**(3): 400. [paper](https://doi.org/10.1214/aoms/1177729586), [wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation).\n",
    "\n",
    "The first condition $\\sum \\alpha_k = \\infty$ insures that whatever the starting parameters $\\theta_0$, no matter how far from $\\theta_0$ the minimum is, this procedure can reach it.\n",
    "\n",
    "The second condition $\\sum \\alpha_k^2 < \\infty$ forces the step sizes to be a decreasing sequence and avoids oscillations around the minimum.\n",
    "\n",
    "Here we have\n",
    "$$g(\\theta) = \\frac{1}{N} \\sum_{i=1}^N 2 \\left(f_\\theta(x_i) - y_i\\right) \\nabla_\\theta f_\\theta(x_i).$$\n",
    "\n",
    "To simplify the notation we shall write $\\alpha$ instead of $\\alpha_k$.\n",
    "\n",
    "One complete pass (to compute the $\\sum_{i=1}^N$) over the training set will be called a *training epoch*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Minibatches\n",
    "\n",
    "Notation disambiguation: $\\theta_k$ refers to the $k$th parameter vector in the sequence above, $\\theta_j$ refers to the $j$th component of vector $\\theta$.\n",
    "\n",
    "Now we have a procedure that enables finding some optimal weights for our network. Provided that, for all parameter $\\theta_j$ in the parameter vector $\\theta$, we can compute $\\frac{\\partial f_\\theta}{\\partial \\theta_j}(x)$, then we can calculate:\n",
    "$$g_{j}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N 2 \\left(f_{\\theta}(x_i) - y_i\\right) \\frac{\\partial f_{\\theta}}{\\partial\\theta_j}(x_i)$$\n",
    "And thus we can update $\\theta_j$ with:\n",
    "$$\\theta_j \\leftarrow \\theta_j - \\alpha g_{j}(\\theta)$$\n",
    "\n",
    "So the computation of all the components in $g(\\theta)$ and the update of each element in $\\theta$ can be done in *parallel*.\n",
    "\n",
    "It appears, however, that for large datasets, summing over the $N$ elements is computationally expensive. \n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**<br>\n",
    "Recall that we actually sum together random variables that are i.i.d., to form a Monte Carlo estimator. Do you remember the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)? What is the relationship between the number of samples collected and the standard deviation of the Monte Carlo estimator?\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "The standard deviation of the empirical average over $n$ i.i.d. samples of a random variable $X$ is $\\frac{\\sigma}{\\sqrt{n}}$, where $\\sigma$ is the standard deviation of the law of $X$.\n",
    "</details>\n",
    "\n",
    "So we could define a cheaper (but noiser) version of the gradient estimator by summing only over a random subset of $n$ training points ($n \\ll N$): \n",
    "$$\\nabla_\\theta L(\\theta) \\approx g(\\theta) = \\frac{1}{n} \\sum_{i=1}^n 2 \\left(f_\\theta(x_i) - y_i\\right) \\nabla_\\theta f_\\theta(x_i) $$\n",
    "\n",
    "Such a subset is called a *minibatch*. When $n=1$, the gradient estimate is based on a single example and is thus very (very!) noisy and convergence can be very slow and unstable. When $n\\rightarrow N$ the noise level decreases at the expense of a heavier computational cost. In practice, the noise level decreases quickly enough that we can take $n\\in [50;1000]$ in most cases.\n",
    "\n",
    "Generally, one takes fixed-size minibatches and drops the $\\frac{1}{n}$ term is the gradient estimate: it blends with the step size $\\alpha$, which we will also call the *learning rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Recursive gradient computation\n",
    "\n",
    "For the update above to be feasible, we need to have a differentiable $f_\\theta$. Let's take a close look at the gradient $\\nabla_\\theta f_\\theta(x)$ and write it as:\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta f_\\theta(x) &= \\left[ \\begin{array}{c} \\vdots \\\\ \\frac{\\partial f_\\theta}{\\partial\\theta_j}(x) \\\\ \\vdots \\end{array}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "In the case of a neural network, the $\\theta_j$ are weights and biases. Let's consider neuron number $j$ and let's write $w_{ij}$ its input weights (with the convention that $i=0$ corresponds to the bias). We will write:\n",
    "- $x_{ij}$ the $i$th input to this neuron\n",
    "- $y_j = \\sum_i w_{ij} x_{ij}$ the scalar input to the activation function\n",
    "- $z_j = \\sigma (y_j)$ the neuron's output\n",
    "<img src=\"img/neuronj-0.png\" width=\"300px\"></img>\n",
    "\n",
    "These three quantities have been computed during the *forward pass*, when $x$ was propagated through the network to obtain $f_\\theta(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**<br>\n",
    "Use the chain rule to write $\\frac{\\partial f_\\theta}{\\partial w_{ij}}(x)$ as an expression of the partial derivatives of $f_\\theta$ with respect to $z_j$ and $y_j$. Simplify this expression.\n",
    "</div>\n",
    "\n",
    "Using the chain rule, we can write:\n",
    "$$\\frac{\\partial f_\\theta}{\\partial w_{ij}}(x) = \\frac{\\partial f_\\theta}{\\partial z_j}(x) \\frac{\\partial z_j}{\\partial y_j}(x) \\frac{\\partial y_j}{\\partial w_{ij}}(x).$$\n",
    "\n",
    "Let's decompose this expression. First we take the last (third) term. We have $y_j = \\sum_i w_{ij} x_{ij}$, so:\n",
    "$$\\frac{\\partial y_j}{\\partial w_{ij}}(x) = x_{ij}.$$\n",
    "\n",
    "Then we take the second term. We have $z_j = \\sigma(y_j)$, so:\n",
    "$$\\frac{\\partial z_j}{\\partial y_j}(x) = \\sigma'(y_j).$$\n",
    "\n",
    "And so:\n",
    "$$\\boxed{\\frac{\\partial f_\\theta}{\\partial w_{ij}}(x) = \\frac{\\partial f_\\theta}{\\partial z_j}(x) \\sigma'(y_j) x_{ij}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the first term remains to be calculated in the expression above.\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**<br>\n",
    "Suppose that $j$ is an output neuron. What is $\\frac{\\partial f_\\theta}{\\partial z_j}(x)$? And consequently, what is the value of $\\frac{\\partial f_\\theta}{\\partial w_{ij}}(x)$?\n",
    "</div>\n",
    "\n",
    "Let's assume $j$ is an output neuron. In this case, $z_j$ is the $j$th component of $f_\\theta(x)$, and we have:\n",
    "$$\\frac{\\partial f_\\theta}{\\partial z_j}(x) = 1.$$\n",
    "\n",
    "Consequently, for the neurons of the output layer:\n",
    "$$\\boxed{\\frac{\\partial f_\\theta}{\\partial w_{ij}}(x) = \\sigma'(y_j) x_{ij}}.$$\n",
    "\n",
    "And hence, the update of the input weights $w_{ij}$ for output neuron $j$ (in the output layer) is:\n",
    "$$w_{ij} \\leftarrow w_{ij} - \\alpha \\left(f_\\theta(x) - y\\right)\\sigma'(y_j) x_{ij}$$\n",
    "Note that, in the expression above, $y$ is the value associated to $x$ (not to be confused with pre-neuron value $y_j$) and we have dropped the sum over elements of the mini-batch for clarity."
   ]
  },
  {
   "attachments": {
    "neuronj-1.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the neurons of the layer just preceding the output layer. Let's call $L_j$ the set of indices of the neurons that directly feed on neuron $j$'s output. $z_j$ corresponds to the $x_{jl}$ variable for these neurons and $y_l$ is the scalar input to neuron $l$. \n",
    "\n",
    "<img src=\"attachment:neuronj-1.png\" width=\"300px\"></img>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**<br>\n",
    "Remark that $x_{jl}=z_j$. Take the total derivative of $f_\\theta$ and derive a recurrence relation between $\\frac{\\partial f_\\theta}{\\partial z_j}(x)$ and the $\\frac{\\partial f_\\theta}{\\partial z_l}(x)$ for $l\\in L_j$.\n",
    "</div>\n",
    "\n",
    "We can take the total derivative:\n",
    "$$\\frac{\\partial f_\\theta}{\\partial z_j}(x) = \\sum_{l \\in L_j} \\frac{\\partial f_\\theta}{\\partial y_l}(x) \\frac{\\partial y_l}{\\partial z_j}(x).$$\n",
    "\n",
    "And so, we can write:\n",
    "$$\\frac{\\partial f_\\theta}{\\partial z_j}(x) = \\sum_{l \\in L_j} \\frac{\\partial f_\\theta}{\\partial z_l}(x) \\frac{\\partial z_l}{\\partial y_l}(x) \\frac{\\partial y_l}{\\partial z_j}(x).$$\n",
    "\n",
    "As previously:\n",
    "$$\\frac{\\partial z_l}{\\partial y_l}(x) = \\sigma'(y_l)\\textrm{ and }\\frac{\\partial y_l}{\\partial z_j}(x) = w_{jl}$$\n",
    "\n",
    "So this total derivative turns into:\n",
    "$$\\boxed{\\frac{\\partial f_\\theta}{\\partial z_j}(x) = \\sum_{l \\in L_j} \\frac{\\partial f_\\theta}{\\partial z_l}(x) \\sigma'(y_l) w_{jl}}.$$\n",
    "\n",
    "This provides a recurrence relation between $\\displaystyle \\frac{\\partial f_\\theta}{\\partial z_j}(x)$ and $\\displaystyle \\frac{\\partial f_\\theta}{\\partial z_l}(x)$ for $l \\in L_j$.\n",
    "\n",
    "Let's write $\\boxed{\\displaystyle  \\delta_j = \\frac{\\partial f_\\theta}{\\partial z_j}(x) \\sigma'(y_j)}$. Recall that our goal is to evaluate:\n",
    "$$\\frac{\\partial f_\\theta}{\\partial w_{ij}}(x) = \\frac{\\partial f_\\theta}{\\partial z_j}(x) \\frac{\\partial z_j}{\\partial y_j}(x) \\frac{\\partial y_j}{\\partial w_{ij}}(x) = \\frac{\\partial f_\\theta}{\\partial z_j}(x) \\sigma'(y_j) x_{ij} = \\delta_j x_{ij},$$\n",
    "$$\\boxed{\\frac{\\partial f_\\theta}{\\partial w_{ij}}(x) = \\delta_j x_{ij}}.$$\n",
    "\n",
    "And we have the following recurrence equation between neuron $j$ and its siblings in $L_j$:\n",
    "$$\\delta_j = \\sigma'(y_j)\\sum_{l\\in L_j} \\delta_l w_{jl}$$\n",
    "\n",
    "And, in particular, for output neurons:\n",
    "$$\\delta_j = \\frac{\\partial f_\\theta}{\\partial z_j}(x) \\sigma'(y_j) = \\sigma'(y_j)$$\n",
    "\n",
    "In short:\n",
    "$$\\boxed{\\delta_j = \\left\\{\\begin{array}{ll}\n",
    "\\sigma'(y_j) & \\textrm{for output neurons,}\\\\\n",
    "\\sigma'(y_j)\\sum_{l\\in L_j} \\delta_l w_{jl} & \\textrm{for other neurons.}\n",
    "\\end{array}\\right.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Backpropagation\n",
    "\n",
    "Let's summarize. The key to backpropagation is to remark that in all cases:\n",
    "$$\\frac{\\partial f_\\theta}{\\partial w_{ij}}(x) = \\delta_j x_{ij}.$$\n",
    "\n",
    "With \n",
    "$$\\delta_j = \\frac{\\partial f_\\theta}{\\partial z_j}(x) \\sigma'(y_j).$$\n",
    "\n",
    "If neuron $j$ is an output neuron, then $z_j$ is the $j$th component of $f_\\theta(x)$. So $ \\frac{\\partial f_\\theta}{\\partial z_j}(x) = 1$. Consequently, for those neurons:\n",
    "$$\\delta_j = \\frac{\\partial f_\\theta}{\\partial z_j}(x) \\sigma'(y_j) = \\sigma'(y_j).$$\n",
    "\n",
    "Recursively, once all the $\\delta_j$ for the output layer have been computed, we can compute the $\\delta_j$ for the last hidden layer as:\n",
    "$$\\delta_j = \\sigma'(y_j) \\sum_{l\\in L_j} \\delta_l w_{jl}.$$\n",
    "\n",
    "And the input weights of neuron $j$ can be updated as:\n",
    "$$w_{ij} \\leftarrow w_{ij} - \\alpha \\left(f_\\theta(x) - y\\right) \\delta_j x_{ij}.$$\n",
    "\n",
    "Once, all these weights have been updated and all the $\\delta_j$ have been computed for the corresponding neurons, we can move one layer back in the network, and so on until we reach the input layer.\n",
    "\n",
    "This algorithm is called *Backpropagation* of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "The backpropagation algorithm allows to update the networks weights.<br>\n",
    "\n",
    "**Forward pass:**\n",
    "<ol style=\"list-style-type:none\">\n",
    "<li> Input $x$\n",
    "<li> $\\lambda=$input layer\n",
    "<li> While $\\lambda\\neq$ output layer:\n",
    "    <ol style=\"list-style-type:none\">\n",
    "    <li>For $j$ in $\\lambda$:\n",
    "        <ol  style=\"list-style-type:none\"><li>Compute $y_j=\\sum w_{ij} x_{ij}$ and $z_j=\\sigma(y_j)$</ol>\n",
    "    <li> $\\lambda \\leftarrow$ next layer\n",
    "    <li> $x \\leftarrow z$\n",
    "    </ol>\n",
    "<li> Output $f_\\theta(x)$\n",
    "</ol>\n",
    "\n",
    "**Backpropagation:**\n",
    "<ol style=\"list-style-type:none\">\n",
    "<li> Output difference $\\Delta = f_\\theta(x) - y$\n",
    "<li> For $j$ in output layer $\\delta_j = \\sigma'(y_j)$\n",
    "<li> $\\lambda =$ output layer\n",
    "<li> While layer $\\lambda \\neq$ input layer:\n",
    "    <ol style=\"list-style-type:none\">\n",
    "    <li> For $j$ in $\\lambda$:\n",
    "        <ol style=\"list-style-type:none\">\n",
    "        <li>Compute $\\delta_j = \\sigma'(y_j) \\sum_{l\\in L} \\delta_l w_{jl}$ (only if not output layer)\n",
    "        <li>Update $w_{ij} \\leftarrow w_{ij} - \\alpha \\Delta \\delta_j x_{ij}$</ol>\n",
    "    <li> $\\lambda=$previous layer\n",
    "    </ol>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of things can be remarked to make this computation more streamlined and efficient.\n",
    "\n",
    "1. $\\sigma'(x)=\\sigma(x)\\left(1-\\sigma(x)\\right)$ so one can get $\\sigma'$ for free during the forward pass and store it.\n",
    "2. all operations of the backward pass can be written in matrix form (just as for the forward pass).\n",
    "3. in the notations above, $x_{0j}=1$ since it is the term that will be multiplied by the bias.\n",
    "4. for a given $j$, all $x_{ij}$ in the notations above are really the $z$ value of the layer before neuron $j$.\n",
    "\n",
    "One can easily rewrite the forward pass and the backpropagation as matrix/vector operations.<br>\n",
    "Let $\\lambda$ be the layer number, starting at 0 for the input layer.<br>\n",
    "Let $w_{\\lambda-1}$ denote the $p\\times q$ weight matrix before layer $\\lambda$ where $p$ is the size of layer $\\lambda$ and $q$ is the size of layer $\\lambda-1$ (plus one for the biases).<br>\n",
    "Finally, let $\\circ$ denote the element-wise product (Hadamard product) of two matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Forward pass:**\n",
    "<ol style=\"list-style-type:none\">\n",
    "<li> Input $x$\n",
    "<li> $\\lambda=1$\n",
    "<li> While $\\lambda\\neq$ output layer index:\n",
    "    <ol style=\"list-style-type:none\">\n",
    "    <li> Compute $y_\\lambda = w_{\\lambda-1}^T x$, \n",
    "    <li> Compute $z_\\lambda = \\sigma (y_\\lambda)$ and $s_\\lambda = \\sigma'(y_\\lambda)$\n",
    "    <li> $\\lambda \\leftarrow \\lambda+1$\n",
    "    <li> $x \\leftarrow z_\\lambda$\n",
    "    </ol>\n",
    "<li> Output $f_\\theta(x)$\n",
    "</ol>\n",
    "\n",
    "**Backpropagation:**\n",
    "<ol style=\"list-style-type:none\">\n",
    "<li> Output difference $\\Delta = f_\\theta(x) - y$\n",
    "<li> $\\lambda=$ output layer index\n",
    "<li> $\\delta_\\lambda = s_\\lambda$\n",
    "<li> $w_{\\lambda-1} \\leftarrow w_{\\lambda-1} - \\alpha \\Delta (\\delta_\\lambda \\cdot z_{\\lambda-1}^T)$\n",
    "<li> $\\lambda\\leftarrow \\lambda -1$\n",
    "<li> While $\\lambda \\neq 0$:\n",
    "    <ol style=\"list-style-type:none\">\n",
    "    <li> $\\delta_\\lambda = s_\\lambda \\circ (\\delta_{\\lambda+1}\\cdot w_\\lambda)$\n",
    "    <li> $w_{\\lambda-1} \\leftarrow w_{\\lambda-1} - \\alpha \\Delta (\\delta_\\lambda \\cdot z_{\\lambda-1}^T)$\n",
    "    <li> $\\lambda\\leftarrow \\lambda -1$\n",
    "    </ol>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that backpropagation is a very general principle for propagating gradients in a computational graph. Any activation function can be substituted in the expressions above, and a very similar derivation can be obtained for other loss functions (more on loss functions later).\n",
    "\n",
    "This is actually the core principle of [Automatic Differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) (or autodiff in short). \n",
    "1. Suppose that $F$ is a function graph of depth $D$, that is $F = f_1 \\circ f_2 \\circ \\ldots \\circ f_D$, \n",
    "2. and all the $f_i$ functions are differentiable with respect to their inputs.\n",
    "\n",
    "To retain the vocabulary of \"inputs and parameters\", let's write $(x_i,\\theta_i)$ the input of $f_i$. Then the reasoning we followed while deriving backprop can be applied to $F$: for a given $x_1=x$ and $\\theta=(\\theta_1,\\ldots,\\theta_D)$, the chain rule provides us with $\\nabla_{x_i} F(x,\\theta)$ and $\\nabla_{\\theta_i} F(x,\\theta)$, and we can compute them in a forward and a backward pass. \n",
    "\n",
    "More generally, let's see all scalar inputs and outputs of this function graph as nodes. The elements of $x$ are input nodes, but also all elements of $\\theta$. Then $F$ is a (possibly very large) directed graph where the elements of $x$ and $\\theta$ are inputs (entry points, at different depths of the graph). Let's write them $Y_{1}$ as if they were the output of a zeroth layer and the input of layer 1.\n",
    "\n",
    "Let's write $y_j$ the value of node $j$, and $Y$ the vector of values of node $j$'s parents in the graph. By definition, $y_j = f_j(Y_{j})$. Since we supposed $f_j$ was differentiable with respect to $Y_{j}$, we can store the Jacobian $\\frac{\\partial f_j}{\\partial Y_{j}}$ in each node $j$ during a forward pass. Then we can compute and store $\\nabla_{Y_j} F(Y_1)$  in all nodes $j$ in a backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.6 Backpropagation in practice\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice (collective):**<br>\n",
    "Write a backpropagation pass for the minibatch containing the triplet $(x=(1,2),y=12.3)$, $(x=(3,1),y=3.4)$ and $(x=(2,5),y=5.1)$ on the network defined before.\n",
    "</div>\n",
    "\n",
    "To help fix ideas, the picture below summarizes all the data structures used.\n",
    "\n",
    "<img src=\"img/nn3.png\" width=\"600px\"></img>\n",
    "\n",
    "- in red, the network's data: w[i] and b[i] store the weights and biases,\n",
    "- in blue, what is computed during the forward pass, y[i] for $w^Tx$, s[i] for $\\sigma'(x)$, z[i] for neuron activations,\n",
    "- in green, what is computed during the backward pass, $\\delta$[i] and the weights and biases updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    val = 1.0/(1.0+np.exp(-z))\n",
    "    der = val*(1.-val)\n",
    "    return val, der\n",
    "\n",
    "def forward_pass(x, verbose=False):\n",
    "    z = [np.zeros((x.shape[0], sz)) for sz in sizes]\n",
    "    s = [np.zeros((x.shape[0], sz)) for sz in sizes]\n",
    "    y = [np.zeros((x.shape[0], sz)) for sz in sizes]\n",
    "    z[0] = x.copy()\n",
    "    for i in range(1,len(sizes)):\n",
    "        if verbose:\n",
    "            print(\"# Forward propagation to layer\", i)\n",
    "        y[i] = np.dot(z[i-1],weights[i-1].T) + biases[i-1]\n",
    "        if verbose:\n",
    "            print(\"Neuron inputs:\", y[i])\n",
    "        if i==len(sizes)-1:\n",
    "            s[i] = np.ones((x.shape[0],sizes[-1]))\n",
    "            z[i] = y[i]\n",
    "        else:\n",
    "            v,d  = sigmoid(y[i])\n",
    "            s[i] = d\n",
    "            z[i] = v\n",
    "        if verbose:\n",
    "            print(\"Layer outputs:\", z[i])\n",
    "    return y,s,z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a batch of two training samples: $\\left(x=(1,2),y=12.3\\right)$ and $\\left(x=(3,1),y=3.4\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_value = np.array([[1,2]])\n",
    "#output_value = np.array([[12.3]])\n",
    "input_value = np.array([[1,2],[3,1],[2,5]])\n",
    "output_value = np.array([[12.3],[3.4],[5.1]])\n",
    "y,s,z = forward_pass(input_value, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*z, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(out, y, s, z, alpha):\n",
    "    delta = [np.zeros((out.shape[0], sz)) for sz in sizes]\n",
    "    error = z[len(sizes)-1] - out\n",
    "    for i in range(len(sizes)-1,0,-1):\n",
    "        # compute delta\n",
    "        if i==len(sizes)-1:\n",
    "            delta[i] = s[len(sizes)-1]\n",
    "        else:\n",
    "            delta[i] = np.dot(delta[i+1],weights[i])\n",
    "            delta[i] = np.multiply(delta[i],s[i])\n",
    "        # intermediate delta value that includes the error term \n",
    "        # (useful for minibatches since each element has a different error value)\n",
    "        delta_temp = np.multiply(delta[i],error)\n",
    "        # update weights\n",
    "        grad_w = np.dot(delta_temp.T,z[i-1])\n",
    "        grad_b = np.sum(delta_temp, axis=0)\n",
    "        weights[i-1] -= alpha * grad_w\n",
    "        biases[i-1]  -= alpha * grad_b\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "backward_pass(output_value, y, s, z, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's monitor the training error for various training sets and weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_value = np.array([[1,2]])\n",
    "#output_value = np.array([[12.3]])\n",
    "#input_value = np.array([[1,2],[3,1]])\n",
    "#output_value = np.array([[12.3],[3.4]])\n",
    "input_value = np.array([[1,2],[3,1],[2,5]])\n",
    "output_value = np.array([[12.3],[3.4],[5.1]])\n",
    "\n",
    "sizes = [2,4,3,1]\n",
    "biases = [np.random.randn(1,y) for y in sizes[1:]]\n",
    "weights = [np.random.randn(out,inp) for inp,out in zip(sizes[:-1],sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nsteps = 500\n",
    "training_loss = np.zeros(nsteps)\n",
    "for i in range(nsteps):\n",
    "    y,s,z = forward_pass(input_value, verbose=False)\n",
    "    training_loss[i] = np.mean((z[-1]-output_value)**2)\n",
    "    backward_pass(output_value, y, s, z, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.semilogy(training_loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,s,z = forward_pass(input_value, verbose=False)\n",
    "print(z[-1])\n",
    "print(output_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. It seems to work, we've learned a network that can memorize the three point-wise mappings:\n",
    "$$\\begin{array}{ccc} \n",
    "(1,2) & \\rightarrow & 12.3\\\\\n",
    "(3,1) & \\rightarrow & 3.4 \\\\\n",
    "(2,5) & \\rightarrow & 5.1\n",
    "\\end{array}$$\n",
    "\n",
    "That's a good start. Let's try to generalize this to any number of points. <br>\n",
    "Let's re-initialize and check the learning on the noise-less function $x_0,x_1\\mapsto \\sqrt{x_0+x_1}$.<br>\n",
    "This time we shall visualize both the training and testing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [2,4,3,1]\n",
    "biases = [np.random.randn(1,y) for y in sizes[1:]]\n",
    "weights = [np.random.randn(out,inp) for inp,out in zip(sizes[:-1],sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training and testing sets\n",
    "def func(x):\n",
    "    return np.sqrt(x[:,0]+x[:,1])\n",
    "\n",
    "testing_x  = np.random.uniform(size=(100,2))\n",
    "testing_y  = func(testing_x).reshape(-1,1)\n",
    "training_x = np.random.uniform(size=(100,2))\n",
    "training_y = func(training_x).reshape(-1,1)\n",
    "\n",
    "# compute initial generalization loss\n",
    "y,s,z = forward_pass(testing_x, verbose=False)\n",
    "err = np.mean((z[-1]-testing_y)**2)\n",
    "print(\"Generalization loss estimate:\",err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsteps = 1000\n",
    "training_loss = np.zeros(nsteps)\n",
    "testing_loss  = np.zeros(nsteps)\n",
    "\n",
    "for i in range(nsteps):\n",
    "    _,_,z_test = forward_pass(testing_x, verbose=False)\n",
    "    testing_loss[i]  = np.mean((z_test[-1]-testing_y)**2)\n",
    "    y_train,s_train,z_train = forward_pass(training_x, verbose=False)\n",
    "    training_loss[i] = np.mean((z_train[-1]-training_y)**2)\n",
    "    backward_pass(training_y, y_train, s_train, z_train, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(training_loss, c='b')\n",
    "plt.semilogy(testing_loss, c='r')\n",
    "print(\"last training loss:\", training_loss[-1])\n",
    "print(\"last generalization loss:\", testing_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(size=(10,2))\n",
    "ztrue = func(x).reshape(-1,1)\n",
    "print(ztrue)\n",
    "_,_,zpred = forward_pass(x)\n",
    "print(zpred[-1])\n",
    "print(ztrue-zpred[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obervations.**<br>\n",
    "It (generally) works but it's not so impressive. Sometimes the weight optimization even diverges. But (in theory) we (should) have found the best possible fit for the neural network's weights since what we did was (attempt to) minimize the loss function. So what can go wrong?\n",
    "\n",
    "A few possible answers at this stage:\n",
    "- The $x_0,x_1\\mapsto \\sqrt{x_0+x_1}$ function just cannot be satisfyingly represented with this neural network architecture.\n",
    "- The stochastic gradient descent procedure got stuck in a local minimum of the loss function.\n",
    "- When divergence occured, the (constant) learning rate was not small enough to keep the gradient steps small.\n",
    "\n",
    "What's your opinion on these phenomena and their possible solutions?\n",
    "\n",
    "**Note.**<br>\n",
    "You can find this code wrapped-up as a Python class in `ann.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.7 MLP in scikit-learn\n",
    "\n",
    "Let's train our first neural network with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    x = 15.*x-5.\n",
    "    return x*np.sin(x)\n",
    "\n",
    "sigma_noise = 0.3\n",
    "def observation(x):\n",
    "    return func(x) + np.random.normal(0,sigma_noise,x.shape[0])\n",
    "\n",
    "X = np.linspace(0,1,1000)\n",
    "N = X.shape[0]\n",
    "Y = observation(X)\n",
    "x = np.linspace(0,1,100)\n",
    "\n",
    "fig=plt.figure(figsize=(7,7), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(X, Y, 'r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x, func(x), 'b', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-6, 10)\n",
    "plt.legend(loc='upper left');\n",
    "\n",
    "print(\"nb points:\", N)\n",
    "\n",
    "X=X.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "myNN = MLPRegressor(hidden_layer_sizes=(100,10), activation='tanh', solver='lbfgs', max_iter=5000, learning_rate_init=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNN.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypredict = myNN.predict(X)\n",
    "\n",
    "fig=plt.figure(figsize=(10,10), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(X.ravel(), Y.ravel(), 'r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x, func(x), 'b', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(X, ypredict, 'g', label=u'$f(x) = NN(x)$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-6, 10)\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit unconvincing but very sensitive to the choice of hyperparameters in `MLPRegressor`. We are going to focus on the classification case and will use the PyTorch API in the next part of this class, but this first example contains many warnings. Although appealing for their many good properties (trainable online, scalable to large datasets and dimensions, versatile), neural networks can be hard to tune and are prone to overfitting (or underfitting) if nothing is done to prevent it. Also, the current scikit-learn implementation is maybe not the most adapted tool for neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id=\"sec6\"></a>Neural networks for classification\n",
    "\n",
    "The derivation we have written above can be repeated for other loss functions. In particular, for classification tasks, if one has $K$ classes with $p_k$ the target classes probabilities for input $x$, the cross-entropy loss fonction is commonly used in classification:\n",
    "$$L(\\theta) = \\sum_{k=1}^K p_k \\log f_\\theta(x)$$\n",
    "\n",
    "Scikit-learn offers an easy API for classification as illustrated below, but its flexibility remains limited and PyTorch offers a great API that we shall use in the next part of this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append('.')\n",
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "myNN = MLPClassifier(hidden_layer_sizes=(25))\n",
    "myNN.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNN.score(Xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accuracy is already excellent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

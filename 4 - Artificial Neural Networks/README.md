# Artificial Neural Networks

* [Home](https://supaerodatascience.github.io/deep-learning/)
* [Github repository](https://github.com/SupaeroDataScience/deep-learning/)

This class gives a biological and historical overview of artificial neural
networks and walks through the theoretical foundations of backpropagation and
gradient descent, using an example in numpy.

[Notebook](https://github.com/SupaeroDataScience/deep-learning/blob/main/ANN/Artificial%20neural%20networks.ipynb)

As an additional exercise, this notebook runs the provided backpropagation
code, visualizing neural activations and error at each step.

[Visualizing backprop](https://github.com/SupaeroDataScience/deep-learning/blob/main/ANN/Visualizing%20Backpropagation.ipynb)


## Additional Resources

In class we present a Universal Approximation Theorem for single-layer networks
with sigmoid activation functions. The slides for that proof are here:
[Universal Approximation Theorem](UniversalApproximationTheorem.pdf)

[This online book](http://neuralnetworksanddeeplearning.com/chap4.html) has a
nice interactive visualization of this property of neural networks.

[The deep learning book](https://www.deeplearningbook.org/) is fully available
online and contains many great examples. Notebook versions of those examples are
available [here](https://github.com/hadrienj/deepLearningBook-Notes)

Stanford CS229 Lectures 11 and 12 introduce ANNs, backpropagation, and gradient
descent, with lecture 12 going further in depth on how to resolve common
training problems in ANNs.

<iframe width="560" height="315" src="https://www.youtube.com/embed/MfIjxPh6Pys" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/zUazLXZZA2U" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

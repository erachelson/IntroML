{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><font size=6>Text data pre-processing</font></b></center>\n",
    "\n",
    "In this exercice, we shall load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers.\n",
    "\n",
    "\"What is there to pre-process?\" you might ask. Well, actually, text data comes in a very noisy form that we, humans, have become accustomed to and filter out effortlessly to grasp the core meaning of the text. It has a lot of formatting (fonts, colors, typography...), punctuation, abbreviations, common words, grammatical rules, etc. that we might wish to discard before even starting the data analysis.\n",
    "\n",
    "Here are some pre-processing steps that can be performed on text:\n",
    "1. loading the data, removing attachements, merging title and body;\n",
    "2. tokenizing - splitting the text into atomic \"words\";\n",
    "3. removal of stop-words - very common words;\n",
    "4. removal of non-words - punctuation, numbers, gibberish;\n",
    "3. lemmatization - merge together \"find\", \"finds\", \"finder\".\n",
    "\n",
    "The final goal is to be able to represent a document as a mathematical object, e.g. a vector, that our machine learning black boxes can process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Let's first load the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emails 2893\n",
      "email file: ../data/lingspam_public/bare/part9/9-56msg1.txt\n",
      "email is a spam: False\n",
      "Subject: aiml ' 98 : first call for papers\n",
      "\n",
      "first call for papers aiml ' 98 advances in modal logic ' 98 october 16-18 , 1998 uppsala university , uppsala , sweden advances in modal logic is an initiative aimed at presenting an up-to - date picture of the state of the art in modal logic and its many applications . the initiative consists of a workshops series together with volumes based on those workshops . advances in modal logic ' 98 is the second workshop organized as part of this initiative . aiml ' 98 will be held from october 16-18 , 1998 in uppsala , sweden . the workshop is intended for users of modal logic in cognition , computing , and language , as well as for logicians working in modal logic . topics . aiml ' 98 will be organized around a number of thematic areas : - modal logics of agency and normative systems - algebraic and model-theoretic aspects of modal logic - modal approaches to grammar and natural language semantics - computational aspects of modal logic - philosophical aspects of modal logic - modal logic and belief revision . papers on related subjects will also be considered . special session . during the workshop there will be a special afternoon session on modal logic and belief revision ; this session will be chaired by sven - ove hansson and sten lindstrom . invited speakers . invited speakers include j . van benthem , k . fine , j . horty , m . kracht , and r . parikh . paper submission . authors are invited to submit a detailed abstract of a full paper of at most 10 pages by e-mail to heinrich wansing ( e-mail address : wansing @ rz . uni-leipzig . de ) , using ` aiml98 submission ' as the subject line . the cover page should include title , authors , and the coordinates of the corresponding author . following this it should be indicated which of the thematic areas best describes the content of the paper ( if none is appropriate , please give a set of keywords that best describe the topic of the paper ) . to be considered , submissions must be received no later than june 1 , 1998 . the preliminary version of the full paper to be included in a planned volume from the workshop should be available at the workshop ; the volume will be submitted to csli publications . authors will be notified of the acceptance of their papers by december 1 , 1998 . sponsors . aiml ' 98 is generously sponsored by neurotec hochtechnologie gmbh , the computational logic group at the university of amsterdam , the compulog net network for computational logic , the swedish royal academy of science , and the university of uppsala . important dates submission deadline : june 1 , 1998 notification : august 1 , 1998 workshop : october 16-18 , 1998 preliminary version for workshop volume due at the workshop notification of acceptance for publication : december 1 , 1998 programme committee maarten de rijke , amsterdam krister segerberg , uppsala heinrich wansing , leipzig michael zakharyaschev , moscow programme chair michael zakharyaschev institute of applied mathematics russian academy of sciences miusskaya square 4 125047 moscow russia ( e-mails : mishaz @ math . fu-berlin . de and mz @ spp . keldysh . ru ) aiml steering committee maarten de rijke , heinrich wansing , michael zakharyaschev aiml advisory board johan van benthem , amsterdam max cresswell , wellington luis farinas del cerro , toulouse larry moss , indiana krister segerberg , uppsala colin stirling , edinburgh further information . email enquiries about aiml ' 98 should be directed to krister . segerberg @ filosofi . uu . se . information about the aiml initiative can be obtained on the world - wide web at http : / / www . wins . uva . nl / ~ mdr / aiml .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_switch=1\n",
    "if(data_switch==0):\n",
    "    train_dir = '../data/ling-spam/train-mails/'\n",
    "    email_path = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "else:\n",
    "    train_dir = '../data/lingspam_public/bare/'\n",
    "    email_path = []\n",
    "    email_label = []\n",
    "    for d in os.listdir(train_dir):\n",
    "        folder = os.path.join(train_dir,d)\n",
    "        email_path += [os.path.join(folder,f) for f in os.listdir(folder)]\n",
    "        email_label += [f[0:3]=='spm' for f in os.listdir(folder)]\n",
    "print(\"number of emails\",len(email_path))\n",
    "email_nb = 0 # try 8 for a spam example\n",
    "print(\"email file:\", email_path[email_nb])\n",
    "print(\"email is a spam:\", email_label[email_nb])\n",
    "print(open(email_path[email_nb]).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering out the noise\n",
    "\n",
    "One nice thing about scikit-learn is that is has lots of preprocessing utilities. Like [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for instance, that converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "- To remove stop-words, we set: `stop_words='english'`\n",
    "- To convert all words to lowercase: `lowercase=True`\n",
    "- The default tokenizer in scikit-learn removes punctuation and only keeps words of more than 2 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvect = CountVectorizer(input='filename', stop_words='english', lowercase=True)\n",
    "word_count = countvect.fit_transform(email_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 60618\n",
      "Document - words matrix: (2893, 60618)\n",
      "First words: ['00' '000' '0000' '00001' '00003000140' '00003003958' '00007' '0001'\n",
      " '00010' '00014' '0003' '00036' '000bp' '000s' '000yen' '001' '0010'\n",
      " '0010010034' '0011' '00133' '0014' '00170' '0019' '00198' '002' '002656'\n",
      " '0027' '003' '0030' '0031' '00333' '0037' '0039' '003n7' '004' '0041'\n",
      " '0044' '0049' '005' '0057' '006' '0067' '007' '00710' '0073' '0074'\n",
      " '00799' '008' '009' '00919680' '0094' '00a' '00am' '00arrival' '00b'\n",
      " '00coffee' '00congress' '00d' '00dinner' '00f' '00h' '00hfstahlke' '00i'\n",
      " '00j' '00l' '00m' '00p' '00pm' '00r' '00t' '00tea' '00the' '00uzheb' '01'\n",
      " '0100' '01003' '01006' '0104' '0106' '01075' '0108' '011' '0111' '0117'\n",
      " '0118' '01202' '01222' '01223' '01225' '01232' '01235' '01273' '013'\n",
      " '0131' '01334' '0135' '01364' '0139' '013953' '013a']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names_out()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Even better filtering\n",
    "\n",
    "That's already quite ok, but this pre-processing does not perform lemmatization, the list of stop-words could be better and we could wish to remove non-english words (misspelled, with numbers, etc.).\n",
    "\n",
    "A slightly better preprocessing uses the [Natural Language Toolkit](https://www.nltk.org/https://www.nltk.org/). The one below:\n",
    "- tokenizes;\n",
    "- removes punctuation;\n",
    "- removes stop-words;\n",
    "- removes non-English and misspelled words (optional);\n",
    "- removes 1-character words;\n",
    "- removes non-alphabetical words (numbers and codes essentially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/emmanuel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/emmanuel/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/emmanuel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If this is the first time you run this notebook, run the lines below\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emmanuel/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.wnl.lemmatize(t) for t in word_list]\n",
    "\n",
    "countvect = CountVectorizer(input='filename',tokenizer=LemmaTokenizer(remove_non_words=True))\n",
    "word_count = countvect.fit_transform(email_path)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 14279\n",
      "Document - words matrix: (2893, 14279)\n",
      "First words: ['aa' 'aal' 'aba' 'aback' 'abacus' 'abandon' 'abandoned' 'abandonment'\n",
      " 'abbas' 'abbreviation' 'abdomen' 'abduction' 'abed' 'aberrant'\n",
      " 'aberration' 'abide' 'abiding' 'abigail' 'ability' 'ablative' 'ablaut'\n",
      " 'able' 'abler' 'aboard' 'abolition' 'abord' 'aboriginal' 'aborigine'\n",
      " 'abound' 'abox' 'abreast' 'abridged' 'abroad' 'abrogate' 'abrook'\n",
      " 'abruptly' 'abscissa' 'absence' 'absent' 'absolute' 'absolutely'\n",
      " 'absoluteness' 'absolutist' 'absolutive' 'absolutization' 'absorbed'\n",
      " 'absorption' 'abstract' 'abstraction' 'abstractly' 'abstractness'\n",
      " 'absurd' 'absurdity' 'abu' 'abundance' 'abundant' 'abuse' 'abusive'\n",
      " 'abyss' 'academe' 'academic' 'academically' 'academician' 'academy'\n",
      " 'accelerate' 'accelerated' 'accelerative' 'accent' 'accentuate'\n",
      " 'accentuation' 'accept' 'acceptability' 'acceptable' 'acceptance'\n",
      " 'acceptation' 'accepted' 'acception' 'access' 'accessibility'\n",
      " 'accessible' 'accessibly' 'accidence' 'accident' 'accidental'\n",
      " 'accidentality' 'accidentally' 'acclaim' 'accommodate' 'accommodation'\n",
      " 'accompany' 'accomplish' 'accomplished' 'accomplishment' 'accord'\n",
      " 'accordance' 'according' 'accordingly' 'account' 'accountability'\n",
      " 'accountant']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names_out()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term frequency times inverse document frequency\n",
    "\n",
    "After this first preprocessing, each document is summarized by a vector of size \"number of words in the extracted dictionnary\". For example, the first email in the list has become:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email:\n",
      "Subject: aiml ' 98 : first call for papers\n",
      "\n",
      "first call for papers aiml ' 98 advances in modal logic ' 98 october 16-18 , 1998 uppsala university , uppsala , sweden advances in modal logic is an initiative aimed at presenting an up-to - date picture of the state of the art in modal logic and its many applications . the initiative consists of a workshops series together with volumes based on those workshops . advances in modal logic ' 98 is the second workshop organized as part of this initiative . aiml ' 98 will be held from october 16-18 , 1998 in uppsala , sweden . the workshop is intended for users of modal logic in cognition , computing , and language , as well as for logicians working in modal logic . topics . aiml ' 98 will be organized around a number of thematic areas : - modal logics of agency and normative systems - algebraic and model-theoretic aspects of modal logic - modal approaches to grammar and natural language semantics - computational aspects of modal logic - philosophical aspects of modal logic - modal logic and belief revision . papers on related subjects will also be considered . special session . during the workshop there will be a special afternoon session on modal logic and belief revision ; this session will be chaired by sven - ove hansson and sten lindstrom . invited speakers . invited speakers include j . van benthem , k . fine , j . horty , m . kracht , and r . parikh . paper submission . authors are invited to submit a detailed abstract of a full paper of at most 10 pages by e-mail to heinrich wansing ( e-mail address : wansing @ rz . uni-leipzig . de ) , using ` aiml98 submission ' as the subject line . the cover page should include title , authors , and the coordinates of the corresponding author . following this it should be indicated which of the thematic areas best describes the content of the paper ( if none is appropriate , please give a set of keywords that best describe the topic of the paper ) . to be considered , submissions must be received no later than june 1 , 1998 . the preliminary version of the full paper to be included in a planned volume from the workshop should be available at the workshop ; the volume will be submitted to csli publications . authors will be notified of the acceptance of their papers by december 1 , 1998 . sponsors . aiml ' 98 is generously sponsored by neurotec hochtechnologie gmbh , the computational logic group at the university of amsterdam , the compulog net network for computational logic , the swedish royal academy of science , and the university of uppsala . important dates submission deadline : june 1 , 1998 notification : august 1 , 1998 workshop : october 16-18 , 1998 preliminary version for workshop volume due at the workshop notification of acceptance for publication : december 1 , 1998 programme committee maarten de rijke , amsterdam krister segerberg , uppsala heinrich wansing , leipzig michael zakharyaschev , moscow programme chair michael zakharyaschev institute of applied mathematics russian academy of sciences miusskaya square 4 125047 moscow russia ( e-mails : mishaz @ math . fu-berlin . de and mz @ spp . keldysh . ru ) aiml steering committee maarten de rijke , heinrich wansing , michael zakharyaschev aiml advisory board johan van benthem , amsterdam max cresswell , wellington luis farinas del cerro , toulouse larry moss , indiana krister segerberg , uppsala colin stirling , edinburgh further information . email enquiries about aiml ' 98 should be directed to krister . segerberg @ filosofi . uu . se . information about the aiml initiative can be obtained on the world - wide web at http : / / www . wins . uva . nl / ~ mdr / aiml .\n",
      "\n",
      "Bag of words representation (122 words in dict):\n",
      "{'subject': 2, 'first': 2, 'call': 2, 'modal': 13, 'logic': 13, 'university': 3, 'initiative': 4, 'date': 1, 'picture': 1, 'state': 1, 'art': 1, 'many': 1, 'series': 1, 'together': 1, 'based': 1, 'second': 1, 'workshop': 8, 'organized': 2, 'part': 1, 'intended': 1, 'cognition': 1, 'language': 2, 'well': 1, 'working': 1, 'around': 1, 'number': 1, 'thematic': 2, 'agency': 1, 'normative': 1, 'algebraic': 1, 'model': 1, 'theoretic': 1, 'grammar': 1, 'natural': 1, 'semantics': 1, 'computational': 3, 'philosophical': 1, 'belief': 2, 'revision': 2, 'related': 1, 'also': 1, 'considered': 2, 'special': 2, 'session': 3, 'afternoon': 1, 'sten': 1, 'include': 2, 'van': 2, 'fine': 1, 'paper': 5, 'submission': 3, 'submit': 1, 'detailed': 1, 'abstract': 1, 'full': 2, 'mail': 2, 'address': 1, 'de': 4, 'line': 1, 'cover': 1, 'page': 1, 'title': 1, 'corresponding': 1, 'author': 1, 'following': 1, 'best': 2, 'content': 1, 'none': 1, 'appropriate': 1, 'please': 1, 'give': 1, 'set': 1, 'describe': 1, 'topic': 1, 'must': 1, 'received': 1, 'later': 1, 'june': 2, 'preliminary': 2, 'version': 2, 'included': 1, 'volume': 3, 'available': 1, 'notified': 1, 'acceptance': 2, 'generously': 1, 'group': 1, 'net': 1, 'network': 1, 'royal': 1, 'academy': 2, 'science': 1, 'important': 1, 'deadline': 1, 'notification': 2, 'august': 1, 'due': 1, 'publication': 1, 'committee': 2, 'chair': 1, 'institute': 1, 'applied': 1, 'mathematics': 1, 'square': 1, 'russia': 1, 'math': 1, 'fu': 1, 'berlin': 1, 'steering': 1, 'advisory': 1, 'board': 1, 'wellington': 1, 'larry': 1, 'moss': 1, 'colin': 1, 'information': 2, 'directed': 1, 'se': 1, 'world': 1, 'wide': 1, 'web': 1, 'uva': 1}\n",
      "\n",
      "Vector reprensentation (122 non-zero elements):\n",
      "  (0, 12153)\t2\n",
      "  (0, 4810)\t2\n",
      "  (0, 1733)\t2\n",
      "  (0, 7944)\t13\n",
      "  (0, 7348)\t13\n",
      "  (0, 13427)\t3\n",
      "  (0, 6419)\t4\n",
      "  (0, 3092)\t1\n",
      "  (0, 9248)\t1\n",
      "  (0, 11947)\t1\n",
      "  (0, 765)\t1\n",
      "  (0, 7561)\t1\n",
      "  (0, 11258)\t1\n",
      "  (0, 12856)\t1\n",
      "  (0, 1126)\t1\n",
      "  (0, 11143)\t1\n",
      "  (0, 14174)\t8\n",
      "  (0, 8677)\t2\n",
      "  (0, 8911)\t1\n",
      "  (0, 6528)\t1\n",
      "  (0, 2261)\t1\n",
      "  (0, 7019)\t2\n",
      "  (0, 14001)\t1\n",
      "  (0, 14171)\t1\n",
      "  (0, 752)\t1\n",
      "  :\t:\n",
      "  (0, 9936)\t1\n",
      "  (0, 2372)\t2\n",
      "  (0, 1960)\t1\n",
      "  (0, 6485)\t1\n",
      "  (0, 658)\t1\n",
      "  (0, 7652)\t1\n",
      "  (0, 11892)\t1\n",
      "  (0, 10909)\t1\n",
      "  (0, 7648)\t1\n",
      "  (0, 5120)\t1\n",
      "  (0, 1250)\t1\n",
      "  (0, 11983)\t1\n",
      "  (0, 253)\t1\n",
      "  (0, 1418)\t1\n",
      "  (0, 14002)\t1\n",
      "  (0, 7030)\t1\n",
      "  (0, 8054)\t1\n",
      "  (0, 2286)\t1\n",
      "  (0, 6390)\t2\n",
      "  (0, 3522)\t1\n",
      "  (0, 11123)\t1\n",
      "  (0, 14175)\t1\n",
      "  (0, 14071)\t1\n",
      "  (0, 13971)\t1\n",
      "  (0, 13635)\t1\n"
     ]
    }
   ],
   "source": [
    "mail_number = 0\n",
    "text = open(email_path[mail_number]).read()\n",
    "print(\"Original email:\")\n",
    "print(text)\n",
    "#print(LemmaTokenizer()(text))\n",
    "#print(len(set(LemmaTokenizer()(text))))\n",
    "#print(len([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(len([word_count2[mail_number, i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(set([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]])-set(LemmaTokenizer()(text)))\n",
    "emailBagOfWords = {feat2word[i]: word_count[mail_number, i] for i in word_count[mail_number, :].nonzero()[1]}\n",
    "print(\"Bag of words representation (\", len(emailBagOfWords), \" words in dict):\", sep='')\n",
    "print(emailBagOfWords)\n",
    "print(\"\\nVector reprensentation (\", word_count[mail_number, :].nonzero()[1].shape[0], \" non-zero elements):\", sep='')\n",
    "print(word_count[mail_number, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting words is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called `tf` for Term Frequencies.\n",
    "\n",
    "Another refinement on top of `tf` is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called `tf–idf` for “Term Frequency times Inverse Document Frequency” and again, scikit-learn does the job for us with the [TfidfTransformer](scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 14279)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer().fit_transform(word_count)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every email in the corpus has a vector representation that filters out unrelevant tokens and retains the significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email 0:\n",
      "  (0, 14175)\t0.025349465817376148\n",
      "  (0, 14174)\t0.22506638374675217\n",
      "  (0, 14171)\t0.02861304122922822\n",
      "  (0, 14071)\t0.030600125583418934\n",
      "  (0, 14002)\t0.06375456355210782\n",
      "  (0, 14001)\t0.02294172630882425\n",
      "  (0, 13971)\t0.024614522450428524\n",
      "  (0, 13851)\t0.09057496564981124\n",
      "  (0, 13731)\t0.06939013872305265\n",
      "  (0, 13668)\t0.0694904308067751\n",
      "  (0, 13635)\t0.051525781519962066\n",
      "  (0, 13427)\t0.0466135924895318\n",
      "  (0, 12893)\t0.031300605458185284\n",
      "  (0, 12856)\t0.030253334925539985\n",
      "  (0, 12845)\t0.028157936923182542\n",
      "  (0, 12725)\t0.04757110280385832\n",
      "  (0, 12718)\t0.0981968155803024\n",
      "  (0, 12171)\t0.03063230621438087\n",
      "  (0, 12170)\t0.08552986404007432\n",
      "  (0, 12153)\t0.018503844147659414\n",
      "  (0, 11991)\t0.06810300050430651\n",
      "  (0, 11983)\t0.06641617566846952\n",
      "  (0, 11947)\t0.0255148471795215\n",
      "  (0, 11892)\t0.05216409819060909\n",
      "  (0, 11784)\t0.05233548640354513\n",
      "  :\t:\n",
      "  (0, 2286)\t0.05321260635579905\n",
      "  (0, 2261)\t0.04222277157587959\n",
      "  (0, 1960)\t0.033836026972513276\n",
      "  (0, 1733)\t0.042065911335269796\n",
      "  (0, 1418)\t0.04256760361612653\n",
      "  (0, 1257)\t0.05753971444386616\n",
      "  (0, 1250)\t0.037541894474365234\n",
      "  (0, 1209)\t0.0963901285566757\n",
      "  (0, 1126)\t0.025027316063493916\n",
      "  (0, 972)\t0.02193190709672924\n",
      "  (0, 944)\t0.029919074785646236\n",
      "  (0, 932)\t0.031335321904449875\n",
      "  (0, 765)\t0.038322212207291595\n",
      "  (0, 752)\t0.032600604933541236\n",
      "  (0, 679)\t0.03633201848588162\n",
      "  (0, 658)\t0.033478420251701696\n",
      "  (0, 428)\t0.01829456367432555\n",
      "  (0, 372)\t0.06169005680407113\n",
      "  (0, 296)\t0.0477744499360884\n",
      "  (0, 291)\t0.04381977121998464\n",
      "  (0, 253)\t0.056780725575071384\n",
      "  (0, 173)\t0.01978469265217566\n",
      "  (0, 73)\t0.0661839457974012\n",
      "  (0, 63)\t0.09435468913552676\n",
      "  (0, 47)\t0.027423083272277896\n"
     ]
    }
   ],
   "source": [
    "print(\"email 0:\")\n",
    "print(tfidf[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility function\n",
    "\n",
    "Let's put all this loading process into a separate file so that we can reuse it in other experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email file: ../data/lingspam_public/bare/part9/8-923msg1.txt\n",
      "email is a spam: False\n",
      "Subject: non - profit linguistic web site\n",
      "\n",
      "the web site linguistic enterprises is now available at http : / / web . gc . cuny . edu / dept / lingu / enter / this is a non-profit site that aims to help academically trained linguists find private sector employment . it offers down-to - earth advice , how-to information , and an opportunity to discuss prospects and problems with others who have found work or are seeking it . one section of the site is designed to match those wanting linguistic jobs in the private sector with companies looking to hire language specialists . the site is maintained by the ph . d . program in linguistics at the graduate school , city university of new york , in conjunction with the linguistic society of america . please tell others about this forum . if you know of potential employers please encourage them to post opportunities for linguists . if you ' re looking for a job , please post your resume to the site . thank you , janet dean fodor , professor , ph . d . program in linguistics , graduate center , cuny president , linguistic society of america steve hoenisch web developer www . criticism . com shoenish @ interport . net p . o . box 3289 new york , ny 10163-3289\n",
      "\n",
      "Bag of words representation (59 words in dictionary):\n",
      "{'subject': 1, 'university': 1, 'language': 1, 'please': 3, 'available': 1, 'net': 1, 'information': 1, 'web': 4, 'new': 2, 'site': 6, 'one': 1, 'know': 1, 'find': 1, 'non': 2, 'tell': 1, 'school': 1, 'thank': 1, 'box': 1, 'found': 1, 'york': 2, 'linguistic': 5, 'help': 1, 'linguistics': 2, 'forum': 1, 'program': 2, 'profit': 2, 'enter': 1, 'academically': 1, 'trained': 1, 'private': 2, 'sector': 2, 'employment': 1, 'earth': 1, 'advice': 1, 'opportunity': 1, 'discus': 1, 'work': 1, 'seeking': 1, 'section': 1, 'designed': 1, 'match': 1, 'wanting': 1, 'looking': 2, 'hire': 1, 'graduate': 2, 'city': 1, 'conjunction': 1, 'society': 2, 'potential': 1, 'encourage': 1, 'post': 2, 'job': 1, 'resume': 1, 'dean': 1, 'professor': 1, 'center': 1, 'president': 1, 'developer': 1, 'criticism': 1}\n"
     ]
    }
   ],
   "source": [
    "spam_data.print_email(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

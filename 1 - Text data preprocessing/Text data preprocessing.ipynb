{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><font size=6>Text data pre-processing</font></b></center>\n",
    "\n",
    "In this exercice, we shall load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers.\n",
    "\n",
    "\"What is there to pre-process?\" you might ask. Well, actually, text data comes in a very noisy form that we, humans, have become accustomed to and filter out effortlessly to grasp the core meaning of the text. It has a lot of formatting (fonts, colors, typography...), punctuation, abbreviations, common words, grammatical rules, etc. that we might wish to discard before even starting the data analysis.\n",
    "\n",
    "Here are some pre-processing steps that can be performed on text:\n",
    "1. loading the data, removing attachements, merging title and body;\n",
    "2. tokenizing - splitting the text into atomic \"words\";\n",
    "3. removal of stop-words - very common words;\n",
    "4. removal of non-words - punctuation, numbers, gibberish;\n",
    "3. lemmatization - merge together \"find\", \"finds\", \"finder\".\n",
    "\n",
    "The final goal is to be able to represent a document as a mathematical object, e.g. a vector, that our machine learning black boxes can process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Let's first load the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emails 2893\n",
      "email file: ../data/lingspam_public/bare/part10/9-902msg1.txt\n",
      "email is a spam: False\n",
      "Subject: new books : generative studies\n",
      "\n",
      "we would like to bring to your attention to two new publications from john benjamins publishing in the field of generative studies : minimal words in a minimal syntax . word formation in swedish . gunlg josefsson 1998 ix , 199 pp . lingvistic aktuell / linguistics today , 19 us / canada : cloth : 1 55619 903 1 price : us $ 75 . 00 rest of the world : cloth : 90 272 2740 3 price : nlg 150 john benjamins publishing web site : http : / / www . benjamins . com for further information via e-mail : service @ benjamins . com in minimal words in a minimal syntax the author combines a detailed description of the morphological structure of words in swedish with a new approach to theoretical morphology based on the minimalist program of chomsky ( 1995 ) ( as developed for syntactic structure ) . the x - bar theoretic approach to word structure of the principles and parameters framework is replaced by a rule free approach incorporating only merge and move as structure building devices . comparative studies in word variation . adverb , pronouns and clause structure in romance and germanic christopher laenzlinger 1998 x , 371 pp . linguistic aktuell / linguistics today , 20 us / canada : cloth : 1 55619 904 x price : us $ 79 . 00 rest of the world : cloth : 90 272 2741 1 price : nlg 158 john benjamins publishing web site : http : / / www . benjamins . com for further information via e-mail : service @ benjamins . com the present book is a typological study in crucial portions of the grammars of french / romance and german / germanic . it starts by asking : \" what do adverbs , pronouns and full noun phrases have in common ? \" the work finds promising solutions to this question within the principles & parameter framework , on the basis of a well-defined formalization of ( i ) xbar - theory , ( ii ) checking theory , ( iii ) clause structure composition , and ( iv ) locality constraints on syntactic operations and relations . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - bernadette martinez - keck tel : ( 215 ) 836-1200 publicity / marketing fax : ( 215 ) 836-1204 john benjamins north america e-mail : bernie @ benjamins . com po box 27519 philadelphia pa 19118-0519 check out the john benjamins web site : http : / / www . benjamins . com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_switch=1\n",
    "if(data_switch==0):\n",
    "    train_dir = '../data/ling-spam/train-mails/'\n",
    "    email_path = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "else:\n",
    "    train_dir = '../data/lingspam_public/bare/'\n",
    "    email_path = []\n",
    "    email_label = []\n",
    "    for d in os.listdir(train_dir):\n",
    "        folder = os.path.join(train_dir,d)\n",
    "        email_path += [os.path.join(folder,f) for f in os.listdir(folder)]\n",
    "        email_label += [f[0:3]=='spm' for f in os.listdir(folder)]\n",
    "print(\"number of emails\",len(email_path))\n",
    "email_nb = 0 # try 8 for a spam example\n",
    "print(\"email file:\", email_path[email_nb])\n",
    "print(\"email is a spam:\", email_label[email_nb])\n",
    "print(open(email_path[email_nb]).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering out the noise\n",
    "\n",
    "One nice thing about scikit-learn is that is has lots of preprocessing utilities. Like [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for instance, that converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "- To remove stop-words, we set: `stop_words='english'`\n",
    "- To convert all words to lowercase: `lowercase=True`\n",
    "- The default tokenizer in scikit-learn removes punctuation and only keeps words of more than 2 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvect = CountVectorizer(input='filename', stop_words='english', lowercase=True)\n",
    "word_count = countvect.fit_transform(email_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 60618\n",
      "Document - words matrix: (2893, 60618)\n",
      "First words: ['00', '000', '0000', '00001', '00003000140', '00003003958', '00007', '0001', '00010', '00014', '0003', '00036', '000bp', '000s', '000yen', '001', '0010', '0010010034', '0011', '00133', '0014', '00170', '0019', '00198', '002', '002656', '0027', '003', '0030', '0031', '00333', '0037', '0039', '003n7', '004', '0041', '0044', '0049', '005', '0057', '006', '0067', '007', '00710', '0073', '0074', '00799', '008', '009', '00919680', '0094', '00a', '00am', '00arrival', '00b', '00coffee', '00congress', '00d', '00dinner', '00f', '00h', '00hfstahlke', '00i', '00j', '00l', '00m', '00p', '00pm', '00r', '00t', '00tea', '00the', '00uzheb', '01', '0100', '01003', '01006', '0104', '0106', '01075', '0108', '011', '0111', '0117', '0118', '01202', '01222', '01223', '01225', '01232', '01235', '01273', '013', '0131', '01334', '0135', '01364', '0139', '013953', '013a']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Even better filtering\n",
    "\n",
    "That's already quite ok, but this pre-processing does not perform lemmatization, the list of stop-words could be better and we could wish to remove non-english words (misspelled, with numbers, etc.).\n",
    "\n",
    "A slightly better preprocessing uses the [Natural Language Toolkit](https://www.nltk.org/https://www.nltk.org/). The one below:\n",
    "- tokenizes;\n",
    "- removes punctuation;\n",
    "- removes stop-words;\n",
    "- removes non-English and misspelled words (optional);\n",
    "- removes 1-character words;\n",
    "- removes non-alphabetical words (numbers and codes essentially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/emmanuel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/emmanuel/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/emmanuel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If this is the first time you run this notebook, run the lines below\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.wnl.lemmatize(t) for t in word_list]\n",
    "\n",
    "countvect = CountVectorizer(input='filename',tokenizer=LemmaTokenizer(remove_non_words=True))\n",
    "word_count = countvect.fit_transform(email_path)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 14279\n",
      "Document - words matrix: (2893, 14279)\n",
      "First words: ['aa', 'aal', 'aba', 'aback', 'abacus', 'abandon', 'abandoned', 'abandonment', 'abbas', 'abbreviation', 'abdomen', 'abduction', 'abed', 'aberrant', 'aberration', 'abide', 'abiding', 'abigail', 'ability', 'ablative', 'ablaut', 'able', 'abler', 'aboard', 'abolition', 'abord', 'aboriginal', 'aborigine', 'abound', 'abox', 'abreast', 'abridged', 'abroad', 'abrogate', 'abrook', 'abruptly', 'abscissa', 'absence', 'absent', 'absolute', 'absolutely', 'absoluteness', 'absolutist', 'absolutive', 'absolutization', 'absorbed', 'absorption', 'abstract', 'abstraction', 'abstractly', 'abstractness', 'absurd', 'absurdity', 'abu', 'abundance', 'abundant', 'abuse', 'abusive', 'abyss', 'academe', 'academic', 'academically', 'academician', 'academy', 'accelerate', 'accelerated', 'accelerative', 'accent', 'accentuate', 'accentuation', 'accept', 'acceptability', 'acceptable', 'acceptance', 'acceptation', 'accepted', 'acception', 'access', 'accessibility', 'accessible', 'accessibly', 'accidence', 'accident', 'accidental', 'accidentality', 'accidentally', 'acclaim', 'accommodate', 'accommodation', 'accompany', 'accomplish', 'accomplished', 'accomplishment', 'accord', 'accordance', 'according', 'accordingly', 'account', 'accountability', 'accountant']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term frequency times inverse document frequency\n",
    "\n",
    "After this first preprocessing, each document is summarized by a vector of size \"number of words in the extracted dictionnary\". For example, the first email in the list has become:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email:\n",
      "Subject: new books : generative studies\n",
      "\n",
      "we would like to bring to your attention to two new publications from john benjamins publishing in the field of generative studies : minimal words in a minimal syntax . word formation in swedish . gunlg josefsson 1998 ix , 199 pp . lingvistic aktuell / linguistics today , 19 us / canada : cloth : 1 55619 903 1 price : us $ 75 . 00 rest of the world : cloth : 90 272 2740 3 price : nlg 150 john benjamins publishing web site : http : / / www . benjamins . com for further information via e-mail : service @ benjamins . com in minimal words in a minimal syntax the author combines a detailed description of the morphological structure of words in swedish with a new approach to theoretical morphology based on the minimalist program of chomsky ( 1995 ) ( as developed for syntactic structure ) . the x - bar theoretic approach to word structure of the principles and parameters framework is replaced by a rule free approach incorporating only merge and move as structure building devices . comparative studies in word variation . adverb , pronouns and clause structure in romance and germanic christopher laenzlinger 1998 x , 371 pp . linguistic aktuell / linguistics today , 20 us / canada : cloth : 1 55619 904 x price : us $ 79 . 00 rest of the world : cloth : 90 272 2741 1 price : nlg 158 john benjamins publishing web site : http : / / www . benjamins . com for further information via e-mail : service @ benjamins . com the present book is a typological study in crucial portions of the grammars of french / romance and german / germanic . it starts by asking : \" what do adverbs , pronouns and full noun phrases have in common ? \" the work finds promising solutions to this question within the principles & parameter framework , on the basis of a well-defined formalization of ( i ) xbar - theory , ( ii ) checking theory , ( iii ) clause structure composition , and ( iv ) locality constraints on syntactic operations and relations . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - bernadette martinez - keck tel : ( 215 ) 836-1200 publicity / marketing fax : ( 215 ) 836-1204 john benjamins north america e-mail : bernie @ benjamins . com po box 27519 philadelphia pa 19118-0519 check out the john benjamins web site : http : / / www . benjamins . com\n",
      "\n",
      "Bag of words representation (82 words in dict):\n",
      "{'subject': 1, 'new': 3, 'generative': 2, 'would': 1, 'like': 1, 'bring': 1, 'attention': 1, 'two': 1, 'field': 1, 'minimal': 4, 'syntax': 2, 'word': 3, 'formation': 1, 'linguistics': 2, 'today': 2, 'u': 4, 'canada': 2, 'cloth': 4, 'price': 4, 'rest': 2, 'world': 2, 'web': 3, 'site': 3, 'information': 2, 'via': 2, 'mail': 3, 'service': 2, 'author': 1, 'detailed': 1, 'description': 1, 'morphological': 1, 'structure': 6, 'approach': 3, 'theoretical': 1, 'morphology': 1, 'based': 1, 'program': 1, 'syntactic': 2, 'bar': 1, 'theoretic': 1, 'framework': 2, 'rule': 1, 'free': 1, 'merge': 1, 'move': 1, 'building': 1, 'comparative': 1, 'variation': 1, 'adverb': 1, 'clause': 2, 'romance': 2, 'germanic': 2, 'linguistic': 1, 'present': 1, 'book': 1, 'typological': 1, 'study': 1, 'crucial': 1, 'german': 1, 'full': 1, 'noun': 1, 'common': 1, 'work': 1, 'promising': 1, 'question': 1, 'within': 1, 'parameter': 1, 'basis': 1, 'well': 1, 'defined': 1, 'formalization': 1, 'theory': 2, 'composition': 1, 'locality': 1, 'keck': 1, 'publicity': 1, 'marketing': 1, 'north': 1, 'po': 1, 'box': 1, 'pa': 1, 'check': 1}\n",
      "\n",
      "Vector reprensentation (82 non-zero elements):\n",
      "  (0, 12153)\t1\n",
      "  (0, 8306)\t3\n",
      "  (0, 5270)\t2\n",
      "  (0, 14188)\t1\n",
      "  (0, 7224)\t1\n",
      "  (0, 1591)\t1\n",
      "  (0, 894)\t1\n",
      "  (0, 13170)\t1\n",
      "  (0, 4742)\t1\n",
      "  (0, 7878)\t4\n",
      "  (0, 12454)\t2\n",
      "  (0, 14161)\t3\n",
      "  (0, 4988)\t1\n",
      "  (0, 7261)\t2\n",
      "  (0, 12852)\t2\n",
      "  (0, 13190)\t4\n",
      "  (0, 1753)\t2\n",
      "  (0, 2211)\t4\n",
      "  (0, 9699)\t4\n",
      "  (0, 10615)\t2\n",
      "  (0, 14175)\t2\n",
      "  (0, 13971)\t3\n",
      "  (0, 11516)\t3\n",
      "  (0, 6390)\t2\n",
      "  (0, 13741)\t2\n",
      "  :\t:\n",
      "  (0, 2978)\t1\n",
      "  (0, 5315)\t1\n",
      "  (0, 5127)\t1\n",
      "  (0, 8447)\t1\n",
      "  (0, 2373)\t1\n",
      "  (0, 14165)\t1\n",
      "  (0, 9816)\t1\n",
      "  (0, 10043)\t1\n",
      "  (0, 14128)\t1\n",
      "  (0, 8871)\t1\n",
      "  (0, 1132)\t1\n",
      "  (0, 14001)\t1\n",
      "  (0, 3206)\t1\n",
      "  (0, 4983)\t1\n",
      "  (0, 12731)\t2\n",
      "  (0, 2451)\t1\n",
      "  (0, 7324)\t1\n",
      "  (0, 6873)\t1\n",
      "  (0, 9938)\t1\n",
      "  (0, 7595)\t1\n",
      "  (0, 8414)\t1\n",
      "  (0, 9373)\t1\n",
      "  (0, 1518)\t1\n",
      "  (0, 8781)\t1\n",
      "  (0, 2025)\t1\n"
     ]
    }
   ],
   "source": [
    "mail_number = 0\n",
    "text = open(email_path[mail_number]).read()\n",
    "print(\"Original email:\")\n",
    "print(text)\n",
    "#print(LemmaTokenizer()(text))\n",
    "#print(len(set(LemmaTokenizer()(text))))\n",
    "#print(len([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(len([word_count2[mail_number, i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(set([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]])-set(LemmaTokenizer()(text)))\n",
    "emailBagOfWords = {feat2word[i]: word_count[mail_number, i] for i in word_count[mail_number, :].nonzero()[1]}\n",
    "print(\"Bag of words representation (\", len(emailBagOfWords), \" words in dict):\", sep='')\n",
    "print(emailBagOfWords)\n",
    "print(\"\\nVector reprensentation (\", word_count[mail_number, :].nonzero()[1].shape[0], \" non-zero elements):\", sep='')\n",
    "print(word_count[mail_number, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting words is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called `tf` for Term Frequencies.\n",
    "\n",
    "Another refinement on top of `tf` is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called `tf–idf` for “Term Frequency times Inverse Document Frequency” and again, scikit-learn does the job for us with the [TfidfTransformer](scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 14279)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer().fit_transform(word_count)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every email in the corpus has a vector representation that filters out unrelevant tokens and retains the significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email 0:\n",
      "  (0, 14188)\t0.037094782848718\n",
      "  (0, 14175)\t0.0934722055784704\n",
      "  (0, 14165)\t0.04002254329922045\n",
      "  (0, 14161)\t0.14091834497600078\n",
      "  (0, 14128)\t0.048171529657830225\n",
      "  (0, 14001)\t0.04229702064162442\n",
      "  (0, 13971)\t0.13614332463320974\n",
      "  (0, 13741)\t0.10881502462586097\n",
      "  (0, 13678)\t0.07011630447737642\n",
      "  (0, 13190)\t0.1599996223643916\n",
      "  (0, 13183)\t0.07826614029245244\n",
      "  (0, 13170)\t0.039909654001890185\n",
      "  (0, 12852)\t0.11905309913954425\n",
      "  (0, 12731)\t0.09519381308974706\n",
      "  (0, 12726)\t0.05419821507889103\n",
      "  (0, 12725)\t0.08770551484025407\n",
      "  (0, 12454)\t0.0972311360436653\n",
      "  (0, 12450)\t0.11685136612701555\n",
      "  (0, 12153)\t0.017057510566715985\n",
      "  (0, 12120)\t0.05072806105260797\n",
      "  (0, 12107)\t0.3095911863729325\n",
      "  (0, 11516)\t0.15067900018468036\n",
      "  (0, 11269)\t0.12264747535784029\n",
      "  (0, 10888)\t0.07379085907896953\n",
      "  (0, 10825)\t0.15653228058490487\n",
      "  :\t:\n",
      "  (0, 4988)\t0.07936700679871678\n",
      "  (0, 4983)\t0.11982022298764217\n",
      "  (0, 4742)\t0.05338579912998475\n",
      "  (0, 3380)\t0.06934082955117414\n",
      "  (0, 3341)\t0.06172322925370016\n",
      "  (0, 3206)\t0.08375155635908944\n",
      "  (0, 2978)\t0.08770551484025407\n",
      "  (0, 2451)\t0.09141933546656017\n",
      "  (0, 2398)\t0.0669843286927422\n",
      "  (0, 2373)\t0.0621323018697505\n",
      "  (0, 2211)\t0.28889940334992037\n",
      "  (0, 2154)\t0.1620766172993042\n",
      "  (0, 2025)\t0.056239834338859665\n",
      "  (0, 1753)\t0.12493354715530974\n",
      "  (0, 1645)\t0.06511028009051853\n",
      "  (0, 1591)\t0.06561053619488988\n",
      "  (0, 1518)\t0.05600696368361317\n",
      "  (0, 1453)\t0.04728198971290137\n",
      "  (0, 1132)\t0.06229871792697179\n",
      "  (0, 1126)\t0.0461421642771011\n",
      "  (0, 1092)\t0.08846375125001522\n",
      "  (0, 944)\t0.05516096333605155\n",
      "  (0, 894)\t0.06405878412687276\n",
      "  (0, 677)\t0.17773380248123674\n",
      "  (0, 238)\t0.10278780691685108\n"
     ]
    }
   ],
   "source": [
    "print(\"email 0:\")\n",
    "print(tfidf[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility function\n",
    "\n",
    "Let's put all this loading process into a separate file so that we can reuse it in other experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email file: ../data/lingspam_public/bare/part10/9-960msg1.txt\n",
      "email is a spam: False\n",
      "Subject: new series : studies in corpus linguistics\n",
      "\n",
      "john benjamins publishing would like to call your attention to a new series ; studies in corpus linguistics studies in corpus linguistics aims to provide insights into the way a corpus can be used , the type of findings that can be obtained , the possible applications of these findings as well as the theoretical changes that corpus work can bring into linguistics and language engineering . the main concern of scl will be to present findings based on , or related to , the cumulative effect of naturally occurring language and on the interpretation of frequency and distributional data . general editor : elena tognini - bonelli consulting editor : wolfgang teubert terms in context jennifer pearson 1998 xii , 246 pp . studies in corpus linguistics , 1 us / canada : cloth : 1 55619 342 4 price : us $ 69 . 00 rest of the world : cloth : 90 272 2269 x price : nlg 138 john benjamins publishing web site : http : / / www . benjamins . com for further information via e-mail : service @ benjamins . com terms in context applies the methodology that has been developed over the last two decades in corpus linguistics to the relatively new and still little developed field of corpus-based terminography . while corpora are already being used by some terminologists fro the identification of terms and retrieval of contextual fragments , his book describes the first attempt to use corpora for terminography in much the same way as large general reference corpora are already being used for general language lexicography . the author goes beyond the standard problem of identifying terms , as opposed to non-terminological lexical items in text and focuses on identifying metalanguage patterns which point to the presence in text of ( parts of ) reusable definitions of terms . the author examines these and shows how the information which they contain can be retrieved and used as input for terminological entries . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - bernadette martinez - keck tel : ( 215 ) 836-1200 publicity / marketing fax : ( 215 ) 836-1204 john benjamins north america e-mail : bernie @ benjamins . com po box 27519 philadelphia pa 19118-0519 check out the john benjamins web site : http : / / www . benjamins . com\n",
      "\n",
      "Bag of words representation (91 words in dictionary):\n",
      "{'already': 2, 'attempt': 1, 'attention': 1, 'author': 2, 'based': 2, 'beyond': 1, 'book': 1, 'box': 1, 'bring': 1, 'call': 1, 'canada': 1, 'check': 1, 'cloth': 2, 'concern': 1, 'consulting': 1, 'contain': 1, 'context': 2, 'contextual': 1, 'corpus': 11, 'cumulative': 1, 'data': 1, 'distributional': 1, 'editor': 2, 'effect': 1, 'engineering': 1, 'field': 1, 'first': 1, 'frequency': 1, 'fro': 1, 'general': 3, 'go': 1, 'identification': 1, 'information': 2, 'input': 1, 'interpretation': 1, 'keck': 1, 'language': 3, 'large': 1, 'last': 1, 'lexical': 1, 'lexicography': 1, 'like': 1, 'linguistics': 6, 'little': 1, 'mail': 2, 'main': 1, 'marketing': 1, 'metalanguage': 1, 'methodology': 1, 'much': 1, 'naturally': 1, 'new': 3, 'non': 1, 'north': 1, 'opposed': 1, 'pa': 1, 'po': 1, 'point': 1, 'possible': 1, 'presence': 1, 'present': 1, 'price': 2, 'problem': 1, 'provide': 1, 'publicity': 1, 'reference': 1, 'related': 1, 'relatively': 1, 'rest': 1, 'retrieval': 1, 'series': 2, 'service': 1, 'site': 2, 'standard': 1, 'still': 1, 'subject': 1, 'terminological': 2, 'text': 2, 'theoretical': 1, 'two': 1, 'type': 1, 'u': 2, 'use': 1, 'used': 4, 'via': 1, 'way': 2, 'web': 2, 'well': 1, 'work': 1, 'world': 1, 'would': 1}\n"
     ]
    }
   ],
   "source": [
    "spam_data.print_email(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

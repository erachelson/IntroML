{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><font size=6>Text data pre-processing</font></b></center>\n",
    "\n",
    "In this exercice, we shall load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers.\n",
    "\n",
    "\"What is there to pre-process?\" you might ask. Well, actually, text data comes in a very noisy form that we, humans, have become accustomed to and filter out effortlessly to grasp the core meaning of the text. It has a lot of formatting (fonts, colors, typography...), punctuation, abbreviations, common words, grammatical rules, etc. that we might wish to discard before even starting the data analysis.\n",
    "\n",
    "Here are some pre-processing steps that can be performed on text:\n",
    "1. loading the data, removing attachements, merging title and body;\n",
    "2. tokenizing - splitting the text into atomic \"words\";\n",
    "3. removal of stop-words - very common words;\n",
    "4. removal of non-words - punctuation, numbers, gibberish;\n",
    "3. lemmatization - merge together \"find\", \"finds\", \"finder\".\n",
    "\n",
    "The final goal is to be able to represent a document as a mathematical object, e.g. a vector, that our machine learning black boxes can process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Let's first load the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emails 2893\n",
      "email file: ../data/lingspam_public/bare/part10/9-742msg2.txt\n",
      "email is a spam: False\n",
      "Subject: computationally - intensive methods in quantitative linguistics\n",
      "\n",
      "second workshop in computationally-intensive methods in quantitative linguistics department of statistics university of glasgow , uk 7 - 9 september 1998 announcement and call for registration in recent years techniques from disciplines such as computer science , articficial intelligence and statistics have found their way into the pages of journals such as the journal of quantitative linguistics , literary and linguistic computing and computers and the humanities . while this influx may bring more advanced methods of analysis to the fields of quantitative linguistics , stylometry and stylistics , the demands upon researchers to understand and use these new techniques are great . familiarity with the appropriate software and the ear of a sympathetic expert are pre-requisites without which the technique may seem out of reach to the average researcher . the humanities advanced technology and information institute and the department of statistics of the university of glasgow are hence supporting this practical workshop in computationally - intensive methods in quantitative linguistics . the workshop is designed to introduce the participants to four such techniques in a practical environment . each half-day session will be divided into an introductory session in a lecture theatre and a longer period spent working with software and practical examples . all of the speakers have published papers using the analyses they will present and their aim in this workshop is to enable the participants to return to their home institutions able to carry out these techniques in the course of their own research . the sessions and speakers are as follows : harald baayen ; max planck institute for psycholinguistics , nijmegen , the netherlands . large number of rare event models walter daelemans ; university of tilburg , the netherlands . linguistics as data mining : using machine learning techniques to discover linguistic generalizations michael oakes ; university of lancaster , unted kingdom . multivariate statistics in corpus linguistics fiona tweedie ; university of glasgow , united kingdom . time series models in linguistics the workshop will be held in the mathematics building of the university of glasgow , commencing on monday 7 september at 1pm . the four workshop sessions will take place on monday afternoon , tuesday 8 september and the morning of wednesday 9 september . there will also be a half day tour on the wednesday afternoon and a reception in the hunterian art gallery on monday evening . accommodation has been arranged in university accommodation with some en suite facilities . the reception , tea and coffee , lunches on 8 and 9 september and evening meals on 7 and 8 september are included in the registration fee . the registration fee , until 15 july , is gbp150 . 00 and gbp100 . 00 for students . participants who are also attending the digital resources in the humanities conference , 9-12 september are eligible for a discount in the registration fees . for more information about the workshop and to register , please consult the web site at http : / / www . stats . gla . ac . uk / ~ cimql , or send email to the conference organisers at cimql @ stats . gla . ac . uk .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_switch=1\n",
    "if(data_switch==0):\n",
    "    train_dir = '../data/ling-spam/train-mails/'\n",
    "    email_path = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "else:\n",
    "    train_dir = '../data/lingspam_public/bare/'\n",
    "    email_path = []\n",
    "    email_label = []\n",
    "    for d in os.listdir(train_dir):\n",
    "        folder = os.path.join(train_dir,d)\n",
    "        email_path += [os.path.join(folder,f) for f in os.listdir(folder)]\n",
    "        email_label += [f[0:3]=='spm' for f in os.listdir(folder)]\n",
    "print(\"number of emails\",len(email_path))\n",
    "email_nb = 0 # try 8 for a spam example\n",
    "print(\"email file:\", email_path[email_nb])\n",
    "print(\"email is a spam:\", email_label[email_nb])\n",
    "print(open(email_path[email_nb]).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering out the noise\n",
    "\n",
    "One nice thing about scikit-learn is that is has lots of preprocessing utilities. Like [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for instance, that converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "- To remove stop-words, we set: `stop_words='english'`\n",
    "- To convert all words to lowercase: `lowercase=True`\n",
    "- The default tokenizer in scikit-learn removes punctuation and only keeps words of more than 2 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvect = CountVectorizer(input='filename', stop_words='english', lowercase=True)\n",
    "word_count = countvect.fit_transform(email_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 60618\n",
      "Document - words matrix: (2893, 60618)\n",
      "First words: ['00', '000', '0000', '00001', '00003000140', '00003003958', '00007', '0001', '00010', '00014', '0003', '00036', '000bp', '000s', '000yen', '001', '0010', '0010010034', '0011', '00133', '0014', '00170', '0019', '00198', '002', '002656', '0027', '003', '0030', '0031', '00333', '0037', '0039', '003n7', '004', '0041', '0044', '0049', '005', '0057', '006', '0067', '007', '00710', '0073', '0074', '00799', '008', '009', '00919680', '0094', '00a', '00am', '00arrival', '00b', '00coffee', '00congress', '00d', '00dinner', '00f', '00h', '00hfstahlke', '00i', '00j', '00l', '00m', '00p', '00pm', '00r', '00t', '00tea', '00the', '00uzheb', '01', '0100', '01003', '01006', '0104', '0106', '01075', '0108', '011', '0111', '0117', '0118', '01202', '01222', '01223', '01225', '01232', '01235', '01273', '013', '0131', '01334', '0135', '01364', '0139', '013953', '013a']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Even better filtering\n",
    "\n",
    "That's already quite ok, but this pre-processing does not perform lemmatization, the list of stop-words could be better and we could wish to remove non-english words (misspelled, with numbers, etc.).\n",
    "\n",
    "A slightly better preprocessing uses the [Natural Language Toolkit](https://www.nltk.org/https://www.nltk.org/). The one below:\n",
    "- tokenizes;\n",
    "- removes punctuation;\n",
    "- removes stop-words;\n",
    "- removes non-English and misspelled words (optional);\n",
    "- removes 1-character words;\n",
    "- removes non-alphabetical words (numbers and codes essentially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.wnl.lemmatize(t) for t in word_list]\n",
    "\n",
    "countvect = CountVectorizer(input='filename',tokenizer=LemmaTokenizer(remove_non_words=True))\n",
    "word_count = countvect.fit_transform(email_path)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 14279\n",
      "Document - words matrix: (2893, 14279)\n",
      "First words: ['aa', 'aal', 'aba', 'aback', 'abacus', 'abandon', 'abandoned', 'abandonment', 'abbas', 'abbreviation', 'abdomen', 'abduction', 'abed', 'aberrant', 'aberration', 'abide', 'abiding', 'abigail', 'ability', 'ablative', 'ablaut', 'able', 'abler', 'aboard', 'abolition', 'abord', 'aboriginal', 'aborigine', 'abound', 'abox', 'abreast', 'abridged', 'abroad', 'abrogate', 'abrook', 'abruptly', 'abscissa', 'absence', 'absent', 'absolute', 'absolutely', 'absoluteness', 'absolutist', 'absolutive', 'absolutization', 'absorbed', 'absorption', 'abstract', 'abstraction', 'abstractly', 'abstractness', 'absurd', 'absurdity', 'abu', 'abundance', 'abundant', 'abuse', 'abusive', 'abyss', 'academe', 'academic', 'academically', 'academician', 'academy', 'accelerate', 'accelerated', 'accelerative', 'accent', 'accentuate', 'accentuation', 'accept', 'acceptability', 'acceptable', 'acceptance', 'acceptation', 'accepted', 'acception', 'access', 'accessibility', 'accessible', 'accessibly', 'accidence', 'accident', 'accidental', 'accidentality', 'accidentally', 'acclaim', 'accommodate', 'accommodation', 'accompany', 'accomplish', 'accomplished', 'accomplishment', 'accord', 'accordance', 'according', 'accordingly', 'account', 'accountability', 'accountant']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term frequency times inverse document frequency\n",
    "\n",
    "After this first preprocessing, each document is summarized by a vector of size \"number of words in the extracted dictionnary\". For example, the first email in the list has become:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email:\n",
      "Subject: computationally - intensive methods in quantitative linguistics\n",
      "\n",
      "second workshop in computationally-intensive methods in quantitative linguistics department of statistics university of glasgow , uk 7 - 9 september 1998 announcement and call for registration in recent years techniques from disciplines such as computer science , articficial intelligence and statistics have found their way into the pages of journals such as the journal of quantitative linguistics , literary and linguistic computing and computers and the humanities . while this influx may bring more advanced methods of analysis to the fields of quantitative linguistics , stylometry and stylistics , the demands upon researchers to understand and use these new techniques are great . familiarity with the appropriate software and the ear of a sympathetic expert are pre-requisites without which the technique may seem out of reach to the average researcher . the humanities advanced technology and information institute and the department of statistics of the university of glasgow are hence supporting this practical workshop in computationally - intensive methods in quantitative linguistics . the workshop is designed to introduce the participants to four such techniques in a practical environment . each half-day session will be divided into an introductory session in a lecture theatre and a longer period spent working with software and practical examples . all of the speakers have published papers using the analyses they will present and their aim in this workshop is to enable the participants to return to their home institutions able to carry out these techniques in the course of their own research . the sessions and speakers are as follows : harald baayen ; max planck institute for psycholinguistics , nijmegen , the netherlands . large number of rare event models walter daelemans ; university of tilburg , the netherlands . linguistics as data mining : using machine learning techniques to discover linguistic generalizations michael oakes ; university of lancaster , unted kingdom . multivariate statistics in corpus linguistics fiona tweedie ; university of glasgow , united kingdom . time series models in linguistics the workshop will be held in the mathematics building of the university of glasgow , commencing on monday 7 september at 1pm . the four workshop sessions will take place on monday afternoon , tuesday 8 september and the morning of wednesday 9 september . there will also be a half day tour on the wednesday afternoon and a reception in the hunterian art gallery on monday evening . accommodation has been arranged in university accommodation with some en suite facilities . the reception , tea and coffee , lunches on 8 and 9 september and evening meals on 7 and 8 september are included in the registration fee . the registration fee , until 15 july , is gbp150 . 00 and gbp100 . 00 for students . participants who are also attending the digital resources in the humanities conference , 9-12 september are eligible for a discount in the registration fees . for more information about the workshop and to register , please consult the web site at http : / / www . stats . gla . ac . uk / ~ cimql , or send email to the conference organisers at cimql @ stats . gla . ac . uk .\n",
      "\n",
      "Bag of words representation (116 words in dict):\n",
      "{'send': 1, 'site': 1, 'web': 1, 'consult': 1, 'please': 1, 'register': 1, 'discount': 1, 'eligible': 1, 'conference': 2, 'digital': 1, 'fee': 2, 'included': 1, 'coffee': 1, 'tea': 1, 'suite': 1, 'en': 1, 'accommodation': 2, 'evening': 2, 'gallery': 1, 'art': 1, 'reception': 2, 'tour': 1, 'also': 2, 'morning': 1, 'afternoon': 2, 'place': 1, 'take': 1, 'building': 1, 'mathematics': 1, 'series': 1, 'time': 1, 'united': 1, 'corpus': 1, 'kingdom': 2, 'discover': 1, 'learning': 1, 'machine': 1, 'mining': 1, 'data': 1, 'walter': 1, 'event': 1, 'rare': 1, 'number': 1, 'large': 1, 'research': 1, 'course': 1, 'carry': 1, 'able': 1, 'home': 1, 'return': 1, 'enable': 1, 'aim': 1, 'present': 1, 'working': 1, 'spent': 1, 'period': 1, 'longer': 1, 'lecture': 1, 'introductory': 1, 'divided': 1, 'session': 4, 'day': 2, 'half': 2, 'environment': 1, 'four': 2, 'introduce': 1, 'designed': 1, 'practical': 3, 'supporting': 1, 'hence': 1, 'institute': 2, 'information': 2, 'technology': 1, 'researcher': 1, 'average': 1, 'reach': 1, 'seem': 1, 'technique': 1, 'without': 1, 'expert': 1, 'sympathetic': 1, 'ear': 1, 'appropriate': 1, 'familiarity': 1, 'great': 1, 'new': 1, 'use': 1, 'understand': 1, 'upon': 1, 'stylistics': 1, 'analysis': 2, 'advanced': 2, 'bring': 1, 'may': 2, 'influx': 1, 'linguistic': 2, 'literary': 1, 'journal': 1, 'way': 1, 'found': 1, 'intelligence': 1, 'science': 1, 'computer': 1, 'recent': 1, 'registration': 4, 'call': 1, 'announcement': 1, 'university': 7, 'statistic': 4, 'department': 2, 'workshop': 7, 'second': 1, 'linguistics': 8, 'quantitative': 5, 'intensive': 3, 'subject': 1}\n",
      "\n",
      "Vector reprensentation (116 non-zero elements):\n",
      "  (0, 11220)\t1\n",
      "  (0, 11516)\t1\n",
      "  (0, 13971)\t1\n",
      "  (0, 2663)\t1\n",
      "  (0, 9344)\t1\n",
      "  (0, 10378)\t1\n",
      "  (0, 3571)\t1\n",
      "  (0, 4029)\t1\n",
      "  (0, 2543)\t2\n",
      "  (0, 3485)\t1\n",
      "  (0, 4691)\t2\n",
      "  (0, 6245)\t1\n",
      "  (0, 2258)\t1\n",
      "  (0, 12578)\t1\n",
      "  (0, 12258)\t1\n",
      "  (0, 4101)\t1\n",
      "  (0, 88)\t2\n",
      "  (0, 4350)\t2\n",
      "  (0, 5192)\t1\n",
      "  (0, 765)\t1\n",
      "  (0, 10238)\t2\n",
      "  (0, 12926)\t1\n",
      "  (0, 428)\t2\n",
      "  (0, 8031)\t1\n",
      "  (0, 291)\t2\n",
      "  :\t:\n",
      "  (0, 231)\t2\n",
      "  (0, 1591)\t1\n",
      "  (0, 7673)\t2\n",
      "  (0, 6385)\t1\n",
      "  (0, 7257)\t2\n",
      "  (0, 7294)\t1\n",
      "  (0, 6817)\t1\n",
      "  (0, 13952)\t1\n",
      "  (0, 5029)\t1\n",
      "  (0, 6522)\t1\n",
      "  (0, 11085)\t1\n",
      "  (0, 2475)\t1\n",
      "  (0, 10235)\t1\n",
      "  (0, 10383)\t4\n",
      "  (0, 1733)\t1\n",
      "  (0, 559)\t1\n",
      "  (0, 13427)\t7\n",
      "  (0, 11956)\t4\n",
      "  (0, 3298)\t2\n",
      "  (0, 14174)\t7\n",
      "  (0, 11143)\t1\n",
      "  (0, 7261)\t8\n",
      "  (0, 10026)\t5\n",
      "  (0, 6536)\t3\n",
      "  (0, 12153)\t1\n"
     ]
    }
   ],
   "source": [
    "mail_number = 0\n",
    "text = open(email_path[mail_number]).read()\n",
    "print(\"Original email:\")\n",
    "print(text)\n",
    "#print(LemmaTokenizer()(text))\n",
    "#print(len(set(LemmaTokenizer()(text))))\n",
    "#print(len([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(len([word_count2[mail_number, i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(set([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]])-set(LemmaTokenizer()(text)))\n",
    "emailBagOfWords = {feat2word[i]: word_count[mail_number, i] for i in word_count[mail_number, :].nonzero()[1]}\n",
    "print(\"Bag of words representation (\", len(emailBagOfWords), \" words in dict):\", sep='')\n",
    "print(emailBagOfWords)\n",
    "print(\"\\nVector reprensentation (\", word_count[mail_number, :].nonzero()[1].shape[0], \" non-zero elements):\", sep='')\n",
    "print(word_count[mail_number, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting words is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called `tf` for Term Frequencies.\n",
    "\n",
    "Another refinement on top of `tf` is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called `tf–idf` for “Term Frequency times Inverse Document Frequency” and again, scikit-learn does the job for us with the [TfidfTransformer](scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 14279)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer().fit_transform(word_count)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every email in the corpus has a vector representation that filters out unrelevant tokens and retains the significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email 0:\n",
      "  (0, 12153)\t0.013939760887416943\n",
      "  (0, 6536)\t0.22891404217218955\n",
      "  (0, 10026)\t0.361472307112444\n",
      "  (0, 7261)\t0.20876400784744084\n",
      "  (0, 11143)\t0.03913002542983131\n",
      "  (0, 14174)\t0.2967167367673084\n",
      "  (0, 3298)\t0.07427438096333842\n",
      "  (0, 11956)\t0.27673557469439863\n",
      "  (0, 13427)\t0.16387500519915182\n",
      "  (0, 559)\t0.052274648123125184\n",
      "  (0, 1733)\t0.03169010400463821\n",
      "  (0, 10383)\t0.17228774540383035\n",
      "  (0, 10235)\t0.047746712417608375\n",
      "  (0, 2475)\t0.04121540875167929\n",
      "  (0, 11085)\t0.042840245240789034\n",
      "  (0, 6522)\t0.05529134507359821\n",
      "  (0, 5029)\t0.04370929357396549\n",
      "  (0, 13952)\t0.037264961870147446\n",
      "  (0, 6817)\t0.049484198300220786\n",
      "  (0, 7294)\t0.06582215226645458\n",
      "  (0, 7257)\t0.06662524802364263\n",
      "  (0, 6385)\t0.10006846150314704\n",
      "  (0, 7673)\t0.05905307423357691\n",
      "  (0, 1591)\t0.053618327403332396\n",
      "  (0, 231)\t0.11525709340981356\n",
      "  :\t:\n",
      "  (0, 291)\t0.13204545565201734\n",
      "  (0, 8031)\t0.06602272782600867\n",
      "  (0, 428)\t0.0551284027523503\n",
      "  (0, 12926)\t0.07261492980945801\n",
      "  (0, 10238)\t0.12623132770058754\n",
      "  (0, 765)\t0.057739621084526675\n",
      "  (0, 5192)\t0.08825733192892943\n",
      "  (0, 4350)\t0.13009504365761435\n",
      "  (0, 88)\t0.11460107439489338\n",
      "  (0, 4101)\t0.058308669927086484\n",
      "  (0, 12258)\t0.06279146875273227\n",
      "  (0, 12578)\t0.06943967627912438\n",
      "  (0, 2258)\t0.05785158763739322\n",
      "  (0, 6245)\t0.050050677364749996\n",
      "  (0, 4691)\t0.10101551930888651\n",
      "  (0, 3485)\t0.06643275035255435\n",
      "  (0, 2543)\t0.06955835271707901\n",
      "  (0, 4029)\t0.084000384754781\n",
      "  (0, 3571)\t0.07630468072406318\n",
      "  (0, 10378)\t0.054211598251791074\n",
      "  (0, 9344)\t0.02684086726462774\n",
      "  (0, 2663)\t0.06707132433119409\n",
      "  (0, 13971)\t0.037086408054331964\n",
      "  (0, 11516)\t0.0410460292572045\n",
      "  (0, 11220)\t0.032615019155928335\n"
     ]
    }
   ],
   "source": [
    "print(\"email 0:\")\n",
    "print(tfidf[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility function\n",
    "\n",
    "Let's put all this loading process into a separate file so that we can reuse it in other experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email file: ../data/lingspam_public/bare/part10/spmsgc81.txt\n",
      "email is a spam: True\n",
      "Subject: adv : y2k . . . pristine , remote wilderness acreage .\n",
      "\n",
      "the perfect get-a - way is just a click away . . . http : / / www . envst100 . com - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - note : we mail to people whom we feel are interested in what we have to offer and do not allow anyone else to use our list . once in a while we ' ll notify you of web sites , products , services and promotions that could be of special interest to you . we respect your right to privacy . if you do not wish to receive these notices and prefer never to hear from us , simply click on the link provided and put remove in the subject line mailto : tmedia1 @ idmail . com - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - error - - - - - - - - - - - - - - - - - - - - - -\n",
      "\n",
      "Bag of words representation (42 words in dictionary):\n",
      "{'remove': 1, 'put': 1, 'provided': 1, 'link': 1, 'simply': 1, 'hear': 1, 'never': 1, 'prefer': 1, 'receive': 1, 'wish': 1, 'privacy': 1, 'respect': 1, 'notify': 1, 'list': 1, 'anyone': 1, 'allow': 1, 'offer': 1, 'feel': 1, 'people': 1, 'note': 1, 'away': 1, 'get': 1, 'perfect': 1, 'acreage': 1, 'wilderness': 1, 'remote': 1, 'pristine': 1, 'else': 1, 'right': 1, 'click': 2, 'line': 1, 'error': 1, 'interest': 1, 'interested': 1, 'could': 1, 'special': 1, 'mail': 1, 'u': 1, 'web': 1, 'use': 1, 'way': 1, 'subject': 2}\n"
     ]
    }
   ],
   "source": [
    "spam_data.print_email(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
